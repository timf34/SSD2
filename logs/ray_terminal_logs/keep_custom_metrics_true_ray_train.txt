Here are the log outputs from when we set config["keep_per_episode_custom_metrics"] = True  # keep custom metrics + log pure metric, not mean, max, mean, etc.

As we can see, it gets stored as a list, rather than as just one value... this might be why its not getting logged to wandb

/usr/bin/python3 /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_train.py
/home/timf34/.local/lib/python3.8/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  'hamming': pil_image.HAMMING,
/home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  'box': pil_image.BOX,
/home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  'lanczos': pil_image.LANCZOS,
/home/timf34/.local/lib/python3.8/site-packages/ray/_private/ray_option_utils.py:266: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/releases-2.0.0/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.
  warnings.warn(
2022-08-29 17:18:55,123	INFO worker.py:1518 -- Started a local Ray instance.
(pid=17917)
(pid=18481) /home/timf34/.local/lib/python3.8/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
(pid=18481)   import imp
(pid=18481) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
(pid=18481)   'nearest': pil_image.NEAREST,
(pid=18481) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
(pid=18481)   'bilinear': pil_image.BILINEAR,
(pid=18481) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
(pid=18481)   'bicubic': pil_image.BICUBIC,
(pid=18481) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
(pid=18481)   'hamming': pil_image.HAMMING,
(pid=18481) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
(pid=18481)   'box': pil_image.BOX,
(pid=18481) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
(pid=18481)   'lanczos': pil_image.LANCZOS,
(RolloutWorker pid=18481) 2022-08-29 17:19:13,910	WARNING env.py:235 -- Your MultiAgentEnv <CleanupEnv instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.
(RolloutWorker pid=18481) episode 1847451588 (env-idx=0) started.
(RolloutWorker pid=18481) agent id  agent-0
(RolloutWorker pid=18481) agent id  agent-1
(RolloutWorker pid=18481) agent id  agent-2
(RolloutWorker pid=18481) agent id  agent-3
(RolloutWorker pid=18481) agent id  agent-4
(RolloutWorker pid=18481) 2022-08-29 17:19:16,023	WARNING deprecation.py:47 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!
{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': 'cleanup', 'env_config': {'func_create': <function get_env_creator.<locals>.env_creator at 0x7efbb96f4040>, 'env_name': 'cleanup'}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'num_workers': 1, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': True, 'enable_connectors': False, 'rollout_fragment_length': 10, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'horizon': 1000, 'soft_horizon': False, 'no_done_at_end': False, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 0.0001, 'train_batch_size': 32, 'model': {'conv_filters': [[6, [3, 3], 1]], 'conv_activation': 'relu', 'fcnet_hiddens': [32, 32], 'use_lstm': True, 'lstm_use_prev_action': True, 'lstm_use_prev_reward': False, 'lstm_cell_size': 128}, 'optimizer': {}, 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': {}, 'off_policy_estimation_methods': {}, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': True, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 5, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'monitor': -1, 'evaluation_num_episodes': -1, 'metrics_smoothing_episodes': -1, 'timesteps_per_iteration': -1, 'min_iter_time_s': -1, 'collect_metrics_timeout': -1, 'buffer_size': -1, 'prioritized_replay': -1, 'learning_starts': -1, 'replay_batch_size': -1, 'replay_sequence_length': None, 'prioritized_replay_alpha': -1, 'prioritized_replay_beta': -1, 'prioritized_replay_eps': -1, 'min_time_s_per_reporting': -1, 'min_train_timesteps_per_reporting': -1, 'min_sample_timesteps_per_reporting': -1, 'input_evaluation': -1, 'use_critic': True, 'use_gae': True, 'grad_clip': 40.0, 'lr_schedule': None, 'vf_loss_coeff': 0.5, 'entropy_coeff': 0.01, 'entropy_coeff_schedule': None, 'lambda': 1.0, 'input': 'sampler', 'multiagent': {'policies': {'agent-0': (None, Dict(curr_obs:Box(0, 255, (15, 15, 3), uint8), other_agent_actions:Box(0, 9, (4,), uint8), prev_visible_agents:Box(0, 1, (4,), uint8), visible_agents:Box(0, 1, (4,), uint8)), Discrete(9), {'custom_model': 'A2C'}), 'agent-1': (None, Dict(curr_obs:Box(0, 255, (15, 15, 3), uint8), other_agent_actions:Box(0, 9, (4,), uint8), prev_visible_agents:Box(0, 1, (4,), uint8), visible_agents:Box(0, 1, (4,), uint8)), Discrete(9), {'custom_model': 'A2C'}), 'agent-2': (None, Dict(curr_obs:Box(0, 255, (15, 15, 3), uint8), other_agent_actions:Box(0, 9, (4,), uint8), prev_visible_agents:Box(0, 1, (4,), uint8), visible_agents:Box(0, 1, (4,), uint8)), Discrete(9), {'custom_model': 'A2C'}), 'agent-3': (None, Dict(curr_obs:Box(0, 255, (15, 15, 3), uint8), other_agent_actions:Box(0, 9, (4,), uint8), prev_visible_agents:Box(0, 1, (4,), uint8), visible_agents:Box(0, 1, (4,), uint8)), Discrete(9), {'custom_model': 'A2C'}), 'agent-4': (None, Dict(curr_obs:Box(0, 255, (15, 15, 3), uint8), other_agent_actions:Box(0, 9, (4,), uint8), prev_visible_agents:Box(0, 1, (4,), uint8), visible_agents:Box(0, 1, (4,), uint8)), Discrete(9), {'custom_model': 'A2C'})}, 'policy_mapping_fn': <function training_script.<locals>.policy_mapping_fn at 0x7efbb96f40d0>}, 'callbacks': <class 'custom_callback.CustomCallback'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1}
2022-08-29 17:19:16,546	INFO trainable.py:160 -- Trainable.setup took 27.914 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2022-08-29 17:19:16,547	WARNING wandb.py:41 -- `ray.tune.integration.wandb.WandbLoggerCallback` is deprecated and will be removed in the future. Please use `ray.air.callbacks.wandb.WandbLoggerCallback` instead.
/home/timf34/.local/lib/python3.8/site-packages/ray/util/placement_group.py:78: DeprecationWarning: placement_group parameter is deprecated. Use scheduling_strategy=PlacementGroupSchedulingStrategy(...) instead, see the usage at https://docs.ray.io/en/releases-2.0.0/ray-core/package-ref.html#ray-remote.
  return bundle_reservation_check.options(
/home/timf34/.local/lib/python3.8/site-packages/ray/_private/ray_option_utils.py:266: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/releases-2.0.0/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.
  warnings.warn(
/home/timf34/.local/lib/python3.8/site-packages/ray/actor.py:637: DeprecationWarning: placement_group parameter is deprecated. Use scheduling_strategy=PlacementGroupSchedulingStrategy(...) instead, see the usage at https://docs.ray.io/en/releases-2.0.0/ray-core/package-ref.html#ray-remote.
  return actor_cls._remote(args=args, kwargs=kwargs, **updated_options)
/home/timf34/.local/lib/python3.8/site-packages/ray/actor.py:637: DeprecationWarning: placement_group_bundle_index parameter is deprecated. Use scheduling_strategy=PlacementGroupSchedulingStrategy(...) instead, see the usage at https://docs.ray.io/en/releases-2.0.0/ray-core/package-ref.html#ray-remote.
  return actor_cls._remote(args=args, kwargs=kwargs, **updated_options)
/home/timf34/.local/lib/python3.8/site-packages/ray/actor.py:637: DeprecationWarning: placement_group_capture_child_tasks parameter is deprecated. Use scheduling_strategy=PlacementGroupSchedulingStrategy(...) instead, see the usage at https://docs.ray.io/en/releases-2.0.0/ray-core/package-ref.html#ray-remote.
  return actor_cls._remote(args=args, kwargs=kwargs, **updated_options)
wandb: Currently logged in as: timf34. Use `wandb login --relogin` to force relogin
(pid=17919)
(pid=18555) /home/timf34/.local/lib/python3.8/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
(pid=18555)   import imp
(pid=18555) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
(pid=18555)   'nearest': pil_image.NEAREST,
(pid=18555) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
(pid=18555)   'bilinear': pil_image.BILINEAR,
(pid=18555) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
(pid=18555)   'bicubic': pil_image.BICUBIC,
(pid=18555) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
(pid=18555)   'hamming': pil_image.HAMMING,
(pid=18555) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
(pid=18555)   'box': pil_image.BOX,
(pid=18555) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
(pid=18555)   'lanczos': pil_image.LANCZOS,
(A3C pid=18555) 2022-08-29 17:19:30,347	INFO algorithm.py:351 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.
(A3C pid=18555) /home/timf34/.local/lib/python3.8/site-packages/ray/_private/ray_option_utils.py:266: DeprecationWarning: Setting 'object_store_memory' for actors is deprecated since it doesn't actually reserve the required object store memory. Use object spilling that's enabled by default (https://docs.ray.io/en/releases-2.0.0/ray-core/objects/object-spilling.html) instead to bypass the object store memory size limitation.
(A3C pid=18555)   warnings.warn(
(pid=17918)
(pid=18610) /home/timf34/.local/lib/python3.8/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
(pid=18610)   import imp
(pid=18610) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
(pid=18610)   'nearest': pil_image.NEAREST,
(pid=18610) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
(pid=18610)   'bilinear': pil_image.BILINEAR,
(pid=18610) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
(pid=18610)   'bicubic': pil_image.BICUBIC,
(pid=18610) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
(pid=18610)   'hamming': pil_image.HAMMING,
(pid=18610) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
(pid=18610)   'box': pil_image.BOX,
(pid=18610) /home/timf34/.local/lib/python3.8/site-packages/keras/utils/image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
(pid=18610)   'lanczos': pil_image.LANCZOS,
(RolloutWorker pid=18610) 2022-08-29 17:19:46,236	WARNING env.py:235 -- Your MultiAgentEnv <CleanupEnv instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.
(RolloutWorker pid=18610) 2022-08-29 17:19:46,613	WARNING deprecation.py:47 -- DeprecationWarning: `policy_mapping_fn(agent_id)` has been deprecated. Use `policy_mapping_fn(agent_id, episode, worker, **kwargs)` instead. This will raise an error in the future!
(RolloutWorker pid=18610) episode 1596705530 (env-idx=0) started.
(RolloutWorker pid=18610) agent id  agent-0
(RolloutWorker pid=18610) agent id  agent-1
(RolloutWorker pid=18610) agent id  agent-2
(RolloutWorker pid=18610) agent id  agent-3
(RolloutWorker pid=18610) agent id  agent-4
(A3C pid=18555) 2022-08-29 17:19:46,791	INFO trainable.py:160 -- Trainable.setup took 16.452 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
== Status ==
Current time: 2022-08-29 17:19:46 (running for 00:00:30.02)
Memory usage on this node: 3.5/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+
| Trial name              | status   | loc                 |
|-------------------------+----------+---------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |
+-------------------------+----------+---------------------+


== Status ==
Current time: 2022-08-29 17:19:51 (running for 00:00:35.07)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+
| Trial name              | status   | loc                 |
|-------------------------+----------+---------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |
+-------------------------+----------+---------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 1000
  counters:
    num_agent_steps_sampled: 1000
    num_agent_steps_trained: 1000
    num_env_steps_sampled: 200
    num_env_steps_trained: 200
  custom_metrics: {}
  date: 2022-08-29_17-19-51
  done: false
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  episodes_total: 0
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 1000
    num_agent_steps_trained: 1000
    num_env_steps_sampled: 200
    num_env_steps_trained: 200
  iterations_since_restore: 1
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 1000
  num_agent_steps_trained: 1000
  num_env_steps_sampled: 200
  num_env_steps_sampled_this_iter: 200
  num_env_steps_trained: 200
  num_env_steps_trained_this_iter: 200
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 200
  perf:
    cpu_util_percent: 8.799999999999999
    gpu_util_percent0: 0.0
    ram_util_percent: 23.266666666666666
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
  sampler_results:
    custom_metrics: {}
    episode_len_mean: .nan
    episode_media: {}
    episode_reward_max: .nan
    episode_reward_mean: .nan
    episode_reward_min: .nan
    episodes_this_iter: 0
    hist_stats:
      episode_lengths: []
      episode_reward: []
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf: {}
  time_since_restore: 5.030229568481445
  time_this_iter_s: 5.030229568481445
  time_total_s: 5.030229568481445
  timers:
    apply_grad_throughput: 1591.102
    apply_grad_time_ms: 31.425
    grad_wait_time_ms: 0.15
    synch_weights_time_ms: 5.821
    training_iteration_time_ms: 0.162
  timestamp: 1661789991
  timesteps_since_restore: 0
  timesteps_total: 200
  training_iteration: 1
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

wandb: Network error (ReadTimeout), entering retry loop.
== Status ==
Current time: 2022-08-29 17:19:56 (running for 00:00:40.11)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |      1 |          5.03023 |  200 |      nan |                      0 |                  nan |                  nan |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 2000
  counters:
    num_agent_steps_sampled: 2000
    num_agent_steps_trained: 2000
    num_env_steps_sampled: 400
    num_env_steps_trained: 400
  custom_metrics: {}
  date: 2022-08-29_17-19-56
  done: false
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  episodes_total: 0
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 2000
    num_agent_steps_trained: 2000
    num_env_steps_sampled: 400
    num_env_steps_trained: 400
  iterations_since_restore: 2
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 2000
  num_agent_steps_trained: 2000
  num_env_steps_sampled: 400
  num_env_steps_sampled_this_iter: 200
  num_env_steps_trained: 400
  num_env_steps_trained_this_iter: 200
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 200
  perf:
    cpu_util_percent: 11.066666666666668
    gpu_util_percent0: 0.0
    ram_util_percent: 23.733333333333334
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
  sampler_results:
    custom_metrics: {}
    episode_len_mean: .nan
    episode_media: {}
    episode_reward_max: .nan
    episode_reward_mean: .nan
    episode_reward_min: .nan
    episodes_this_iter: 0
    hist_stats:
      episode_lengths: []
      episode_reward: []
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf: {}
  time_since_restore: 10.087615966796875
  time_this_iter_s: 5.05738639831543
  time_total_s: 10.087615966796875
  timers:
    apply_grad_throughput: 1836.68
    apply_grad_time_ms: 27.223
    grad_wait_time_ms: 0.157
    synch_weights_time_ms: 6.245
    training_iteration_time_ms: 0.169
  timestamp: 1661789996
  timesteps_since_restore: 0
  timesteps_total: 400
  training_iteration: 2
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:20:02 (running for 00:00:45.25)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |      2 |          10.0876 |  400 |      nan |                      0 |                  nan |                  nan |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 3100
  counters:
    num_agent_steps_sampled: 3100
    num_agent_steps_trained: 3100
    num_env_steps_sampled: 620
    num_env_steps_trained: 620
  custom_metrics: {}
  date: 2022-08-29_17-20-02
  done: false
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  episodes_total: 0
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner:
      agent-0:
        learner_stats:
          allreduce_latency: 0.0
          cur_lr: 0.0001
          entropy_coeff: 0.01
          grad_gnorm: 40.0
          policy_entropy: 20.8377685546875
          policy_loss: -637.82568359375
          vf_loss: 7269.0947265625
      agent-1:
        learner_stats:
          allreduce_latency: 0.0
          cur_lr: 0.0001
          entropy_coeff: 0.01
          grad_gnorm: 40.0
          policy_entropy: 20.337038040161133
          policy_loss: -278.2978515625
          vf_loss: 2717.667236328125
      agent-2:
        learner_stats:
          allreduce_latency: 0.0
          cur_lr: 0.0001
          entropy_coeff: 0.01
          grad_gnorm: 40.0
          policy_entropy: 20.972755432128906
          policy_loss: -200.21531677246094
          vf_loss: 2480.868896484375
      agent-3:
        learner_stats:
          allreduce_latency: 0.0
          cur_lr: 0.0001
          entropy_coeff: 0.01
          grad_gnorm: 40.0
          policy_entropy: 20.65064239501953
          policy_loss: 0.647071123123169
          vf_loss: 2.177001476287842
      agent-4:
        learner_stats:
          allreduce_latency: 0.0
          cur_lr: 0.0001
          entropy_coeff: 0.01
          grad_gnorm: 40.0
          policy_entropy: 19.727020263671875
          policy_loss: -80.48246765136719
          vf_loss: 71.23682403564453
    num_agent_steps_sampled: 3100
    num_agent_steps_trained: 3100
    num_env_steps_sampled: 620
    num_env_steps_trained: 620
  iterations_since_restore: 3
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 3100
  num_agent_steps_trained: 3100
  num_env_steps_sampled: 620
  num_env_steps_sampled_this_iter: 220
  num_env_steps_trained: 620
  num_env_steps_trained_this_iter: 220
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 220
  perf:
    cpu_util_percent: 11.033333333333333
    gpu_util_percent0: 0.0
    ram_util_percent: 23.733333333333334
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
  sampler_results:
    custom_metrics: {}
    episode_len_mean: .nan
    episode_media: {}
    episode_reward_max: .nan
    episode_reward_mean: .nan
    episode_reward_min: .nan
    episodes_this_iter: 0
    hist_stats:
      episode_lengths: []
      episode_reward: []
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf: {}
  time_since_restore: 15.122089147567749
  time_this_iter_s: 5.034473180770874
  time_total_s: 15.122089147567749
  timers:
    apply_grad_throughput: 2109.771
    apply_grad_time_ms: 23.699
    grad_wait_time_ms: 0.8
    synch_weights_time_ms: 5.697
    training_iteration_time_ms: 4.199
  timestamp: 1661790002
  timesteps_since_restore: 0
  timesteps_total: 620
  training_iteration: 3
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:20:07 (running for 00:00:50.30)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |      3 |          15.1221 |  620 |      nan |                      0 |                  nan |                  nan |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 4150
  counters:
    num_agent_steps_sampled: 4150
    num_agent_steps_trained: 4150
    num_env_steps_sampled: 830
    num_env_steps_trained: 830
  custom_metrics: {}
  date: 2022-08-29_17-20-07
  done: false
  episode_len_mean: .nan
  episode_media: {}
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  episodes_total: 0
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 4150
    num_agent_steps_trained: 4150
    num_env_steps_sampled: 830
    num_env_steps_trained: 830
  iterations_since_restore: 4
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 4150
  num_agent_steps_trained: 4150
  num_env_steps_sampled: 830
  num_env_steps_sampled_this_iter: 210
  num_env_steps_trained: 830
  num_env_steps_trained_this_iter: 210
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 210
  perf:
    cpu_util_percent: 11.200000000000001
    gpu_util_percent0: 0.0
    ram_util_percent: 23.8
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max: {}
  policy_reward_mean: {}
  policy_reward_min: {}
  sampler_perf: {}
  sampler_results:
    custom_metrics: {}
    episode_len_mean: .nan
    episode_media: {}
    episode_reward_max: .nan
    episode_reward_mean: .nan
    episode_reward_min: .nan
    episodes_this_iter: 0
    hist_stats:
      episode_lengths: []
      episode_reward: []
    num_faulty_episodes: 0
    policy_reward_max: {}
    policy_reward_mean: {}
    policy_reward_min: {}
    sampler_perf: {}
  time_since_restore: 20.238924741744995
  time_this_iter_s: 5.116835594177246
  time_total_s: 20.238924741744995
  timers:
    apply_grad_throughput: 2079.322
    apply_grad_time_ms: 24.046
    grad_wait_time_ms: 0.338
    synch_weights_time_ms: 6.359
    training_iteration_time_ms: 0.359
  timestamp: 1661790007
  timesteps_since_restore: 0
  timesteps_total: 830
  training_iteration: 4
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

(RolloutWorker pid=18610) check agent ids ['agent-0', 'agent-1', 'agent-2', 'agent-3', 'agent-4']
(RolloutWorker pid=18610) checking again ['agent-0', 'agent-1', 'agent-2', 'agent-3', 'agent-4']
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) here we are agent-4 [7, 7, 3, 7, 0, 5, 3, 8, 6, 5, 6, 2, 4, 3, 2, 3, 5, 8, 8, 7, 7, 8, 2, 3, 7, 3, 5, 0, 2, 1, 6, 3, 4, 8, 1, 5, 3, 6, 3, 5, 1, 6, 6, 3, 6, 0, 3, 1, 2, 0, 0, 3, 1, 2, 6, 4, 8, 1, 5, 3, 4, 3, 1, 4, 1, 3, 6, 6, 1, 4, 5, 5, 6, 3, 2, 1, 3, 1, 5, 3, 1, 8, 4, 2, 6, 3, 3, 5, 1, 3, 3, 5, 6, 8, 4, 1, 7, 7, 7, 6, 6, 0, 1, 5, 1, 5, 2, 4, 3, 8, 3, 4, 4, 6, 1, 6, 1, 3, 7, 5, 1, 3, 3, 3, 1, 0, 5, 3, 0, 8, 7, 6, 5, 8, 6, 6, 0, 1, 3, 0, 8, 8, 1, 7, 4, 8, 6, 1, 5, 6, 6, 6, 3, 1, 5, 7, 4, 0, 8, 3, 8, 0, 3, 4, 8, 5, 5, 0, 4, 1, 0, 6, 2, 4, 5, 5, 0, 5, 4, 6, 3, 3, 5, 4, 1, 1, 3, 2, 6, 0, 4, 0, 4, 4, 0, 2, 3, 3, 1, 0, 4, 1, 8, 5, 5, 4, 0, 0, 4, 4, 0, 2, 1, 6, 2, 3, 3, 0, 6, 3, 1, 8, 3, 1, 1, 4, 3, 5, 3, 5, 1, 5, 2, 0, 3, 4, 6, 7, 2, 5, 3, 3, 1, 5, 5, 8, 1, 7, 3, 5, 3, 6, 7, 4, 1, 7, 6, 7, 2, 3, 3, 1, 1, 5, 5, 7, 1, 3, 6, 3, 3, 8, 4, 5, 8, 1, 5, 6, 4, 4, 4, 0, 4, 4, 2, 0, 1, 8, 1, 8, 4, 7, 5, 6, 5, 2, 6, 0, 0, 5, 2, 8, 2, 1, 0, 2, 2, 3, 1, 5, 3, 8, 1, 1, 0, 0, 4, 4, 5, 3, 3, 5, 1, 5, 3, 2, 3, 3, 3, 0, 3, 5, 1, 3, 4, 7, 5, 1, 5, 6, 0, 7, 5, 3, 7, 6, 5, 7, 1, 7, 7, 0, 7, 1, 2, 0, 8, 5, 0, 1, 0, 7, 2, 2, 4, 4, 3, 5, 1, 3, 0, 4, 5, 5, 8, 8, 2, 7, 0, 7, 7, 5, 3, 1, 0, 6, 3, 6, 4, 2, 6, 7, 7, 8, 4, 5, 7, 0, 6, 2, 3, 4, 3, 4, 3, 3, 3, 1, 0, 7, 6, 2, 8, 5, 2, 1, 5, 8, 8, 6, 3, 0, 4, 4, 4, 8, 0, 7, 1, 2, 8, 2, 2, 7, 2, 1, 4, 0, 5, 5, 4, 6, 3, 4, 3, 6, 3, 2, 1, 6, 0, 4, 4, 6, 3, 3, 0, 3, 4, 8, 1, 8, 3, 1, 4, 5, 6, 3, 3, 6, 5, 8, 1, 4, 8, 1, 8, 6, 3, 1, 6, 6, 1, 3, 5, 1, 7, 8, 5, 6, 5, 1, 0, 0, 2, 3, 3, 8, 6, 8, 6, 6, 4, 3, 8, 0, 3, 0, 6, 7, 3, 6, 3, 4, 8, 5, 2, 6, 3, 6, 6, 6, 6, 3, 5, 1, 0, 4, 4, 8, 3, 1, 5, 4, 3, 8, 3, 6, 5, 5, 3, 3, 6, 7, 3, 3, 7, 1, 4, 0, 3, 3, 6, 4, 8, 5, 3, 5, 5, 1, 4, 1, 1, 4, 5, 1, 6, 7, 5, 5, 1, 8, 6, 8, 3, 4, 4, 6, 3, 7, 1, 6, 2, 6, 1, 5, 4, 4, 1, 0, 4, 0, 4, 8, 0, 5, 3, 6, 0, 0, 4, 5, 4, 6, 3, 5, 0, 6, 0, 3, 4, 1, 6, 1, 2, 7, 3, 0, 7, 7, 8, 7, 1, 1, 5, 1, 6, 5, 3, 0, 2, 6, 6, 4, 5, 5, 4, 6, 5, 4, 4, 0, 6, 3, 5, 3, 8, 5, 6, 2, 4, 6, 4, 5, 5, 2, 2, 8, 1, 6, 0, 0, 6, 5, 5, 4, 0, 1, 0, 6, 0, 3, 8, 3, 8, 6, 2, 3, 4, 4, 5, 3, 5, 4, 8, 3, 8, 5, 1, 6, 4, 4, 1, 4, 3, 6, 1, 1, 7, 0, 5, 0, 1, 6, 1, 6, 1, 0, 8, 8, 4, 4, 6, 4, 3, 0, 1, 6, 3, 5, 5, 5, 4, 3, 1, 0, 2, 7, 6, 5, 4, 1, 4, 1, 6, 1, 6, 6, 0, 6, 1, 4, 1, 0, 4, 5, 5, 1, 3, 6, 7, 6, 5, 2, 6, 5, 5, 0, 8, 1, 0, 5, 5, 5, 1, 2, 0, 4, 1, 6, 5, 0, 7, 5, 0, 7, 8, 8, 6, 8, 4, 1, 5, 1, 4, 3, 3, 4, 6, 0, 4, 2, 3, 8, 3, 4, 4, 4, 3, 8, 8, 1, 1, 8, 0, 0, 7, 1, 1, 1, 0, 1, 5, 7, 4, 4, 7, 5, 7, 0, 7, 4, 3, 5, 3, 6, 2, 6, 6, 7, 4, 4, 5, 8, 5, 7, 3, 7, 5, 0, 7, 5, 7, 7, 0, 1, 5, 6, 4, 5, 4, 3, 5, 8, 8, 4, 7, 6, 3, 1, 0, 8, 2, 6, 8, 8, 3, 3, 0, 2, 6, 7, 0, 2, 3, 4, 4, 1, 3, 2, 8, 8, 3, 5, 5, 8, 8, 3, 6, 5, 8, 3, 5, 1, 3, 3, 1, 6, 5, 6, 6, 3, 8, 8, 5, 8, 6, 7, 4, 5, 6, 1, 1, 1, 3, 5, 8, 6, 1, 6, 3, 8, 1, 7, 3, 6, 8, 4, 0, 5, 6, 7, 6, 6, 6, 3, 3, 3, 3, 3, 7, 0, 8, 0, 1, 3, 0, 5, 8, 5, 7, 3, 2, 7, 1, 2, 3, 0, 3, 7, 2, 3, 8, 8, 8, 1, 3, 4, 7, 8, 5, 1, 3, 4, 8, 8, 1, 8, 4, 2, 5, 8, 4, 8, 6, 2, 2, 5, 0, 3, 6, 6, 3, 5, 5, 5, 1, 0, 3, 4]
(RolloutWorker pid=18610) and here {'agent-0-beam_fired': 145, 'agent-0-cleaning': 103.0, 'agent-0-beam_hit': 24, 'agent-0-apples_consumed': 4, 'agent-1-beam_fired': 83, 'agent-1-cleaning': 105.0, 'agent-1-beam_hit': 29, 'agent-1-apples_consumed': 0, 'agent-2-beam_fired': 115, 'agent-2-cleaning': 121.0, 'agent-2-beam_hit': 11, 'agent-2-apples_consumed': 2, 'agent-3-beam_fired': 115, 'agent-3-cleaning': 104.0, 'agent-3-beam_hit': 23, 'agent-3-apples_consumed': 0, 'agent-4-beam_fired': 72, 'agent-4-cleaning': 96.0, 'agent-4-beam_hit': 8, 'agent-4-apples_consumed': 5}
(RolloutWorker pid=18610) episode 987867274 (env-idx=0) started.
(RolloutWorker pid=18610) agent id  agent-0
(RolloutWorker pid=18610) agent id  agent-1
(RolloutWorker pid=18610) agent id  agent-2
(RolloutWorker pid=18610) agent id  agent-3
(RolloutWorker pid=18610) agent id  agent-4
== Status ==
Current time: 2022-08-29 17:20:12 (running for 00:00:55.50)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |      4 |          20.2389 |  830 |      nan |                      0 |                  nan |                  nan |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 5250
  counters:
    num_agent_steps_sampled: 5250
    num_agent_steps_trained: 5250
    num_env_steps_sampled: 1050
    num_env_steps_trained: 1050
  custom_metrics:
    agent-0-apples_consumed:
    - 4
    agent-0-beam_fired:
    - 145
    agent-0-beam_hit:
    - 24
    agent-0-cleaning:
    - 103.0
    agent-1-apples_consumed:
    - 0
    agent-1-beam_fired:
    - 83
    agent-1-beam_hit:
    - 29
    agent-1-cleaning:
    - 105.0
    agent-2-apples_consumed:
    - 2
    agent-2-beam_fired:
    - 115
    agent-2-beam_hit:
    - 11
    agent-2-cleaning:
    - 121.0
    agent-3-apples_consumed:
    - 0
    agent-3-beam_fired:
    - 115
    agent-3-beam_hit:
    - 23
    agent-3-cleaning:
    - 104.0
    agent-4-apples_consumed:
    - 5
    agent-4-beam_fired:
    - 72
    agent-4-beam_hit:
    - 8
    agent-4-cleaning:
    - 96.0
  date: 2022-08-29_17-20-12
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -5779.0
  episode_reward_mean: -5779.0
  episode_reward_min: -5779.0
  episodes_this_iter: 1
  episodes_total: 1
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 5250
    num_agent_steps_trained: 5250
    num_env_steps_sampled: 1050
    num_env_steps_trained: 1050
  iterations_since_restore: 5
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 5250
  num_agent_steps_trained: 5250
  num_env_steps_sampled: 1050
  num_env_steps_sampled_this_iter: 220
  num_env_steps_trained: 1050
  num_env_steps_trained_this_iter: 220
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 220
  perf:
    cpu_util_percent: 11.033333333333333
    gpu_util_percent0: 0.0
    ram_util_percent: 23.8
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -714.0
    agent-3: -1418.0
    agent-4: -518.0
  policy_reward_mean:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -714.0
    agent-3: -1418.0
    agent-4: -518.0
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -714.0
    agent-3: -1418.0
    agent-4: -518.0
  sampler_perf:
    mean_action_processing_ms: 0.18768319518411372
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.1010416700160401
    mean_inference_ms: 20.513967422528413
    mean_raw_obs_processing_ms: 2.478747130113173
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed:
      - 4
      agent-0-beam_fired:
      - 145
      agent-0-beam_hit:
      - 24
      agent-0-cleaning:
      - 103.0
      agent-1-apples_consumed:
      - 0
      agent-1-beam_fired:
      - 83
      agent-1-beam_hit:
      - 29
      agent-1-cleaning:
      - 105.0
      agent-2-apples_consumed:
      - 2
      agent-2-beam_fired:
      - 115
      agent-2-beam_hit:
      - 11
      agent-2-cleaning:
      - 121.0
      agent-3-apples_consumed:
      - 0
      agent-3-beam_fired:
      - 115
      agent-3-beam_hit:
      - 23
      agent-3-cleaning:
      - 104.0
      agent-4-apples_consumed:
      - 5
      agent-4-beam_fired:
      - 72
      agent-4-beam_hit:
      - 8
      agent-4-cleaning:
      - 96.0
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -5779.0
    episode_reward_mean: -5779.0
    episode_reward_min: -5779.0
    episodes_this_iter: 1
    hist_stats:
      episode_lengths:
      - 1000
      episode_reward:
      - -5779.0
      policy_agent-0_reward:
      - -1494.0
      policy_agent-1_reward:
      - -1635.0
      policy_agent-2_reward:
      - -714.0
      policy_agent-3_reward:
      - -1418.0
      policy_agent-4_reward:
      - -518.0
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -714.0
      agent-3: -1418.0
      agent-4: -518.0
    policy_reward_mean:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -714.0
      agent-3: -1418.0
      agent-4: -518.0
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -714.0
      agent-3: -1418.0
      agent-4: -518.0
    sampler_perf:
      mean_action_processing_ms: 0.18768319518411372
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.1010416700160401
      mean_inference_ms: 20.513967422528413
      mean_raw_obs_processing_ms: 2.478747130113173
  time_since_restore: 25.455709218978882
  time_this_iter_s: 5.216784477233887
  time_total_s: 25.455709218978882
  timers:
    apply_grad_throughput: 2255.203
    apply_grad_time_ms: 22.171
    grad_wait_time_ms: 0.199
    synch_weights_time_ms: 5.7
    training_iteration_time_ms: 0.216
  timestamp: 1661790012
  timesteps_since_restore: 0
  timesteps_total: 1050
  training_iteration: 5
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:20:17 (running for 00:01:00.78)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |      5 |          25.4557 | 1050 |    -5779 |                      0 |                -5779 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 6350
  counters:
    num_agent_steps_sampled: 6350
    num_agent_steps_trained: 6350
    num_env_steps_sampled: 1270
    num_env_steps_trained: 1270
  custom_metrics:
    agent-0-apples_consumed:
    - 4
    agent-0-beam_fired:
    - 145
    agent-0-beam_hit:
    - 24
    agent-0-cleaning:
    - 103.0
    agent-1-apples_consumed:
    - 0
    agent-1-beam_fired:
    - 83
    agent-1-beam_hit:
    - 29
    agent-1-cleaning:
    - 105.0
    agent-2-apples_consumed:
    - 2
    agent-2-beam_fired:
    - 115
    agent-2-beam_hit:
    - 11
    agent-2-cleaning:
    - 121.0
    agent-3-apples_consumed:
    - 0
    agent-3-beam_fired:
    - 115
    agent-3-beam_hit:
    - 23
    agent-3-cleaning:
    - 104.0
    agent-4-apples_consumed:
    - 5
    agent-4-beam_fired:
    - 72
    agent-4-beam_hit:
    - 8
    agent-4-cleaning:
    - 96.0
  date: 2022-08-29_17-20-17
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -5779.0
  episode_reward_mean: -5779.0
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 1
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 6350
    num_agent_steps_trained: 6350
    num_env_steps_sampled: 1270
    num_env_steps_trained: 1270
  iterations_since_restore: 6
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 6350
  num_agent_steps_trained: 6350
  num_env_steps_sampled: 1270
  num_env_steps_sampled_this_iter: 220
  num_env_steps_trained: 1270
  num_env_steps_trained_this_iter: 220
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 220
  perf:
    cpu_util_percent: 10.866666666666667
    gpu_util_percent0: 0.0
    ram_util_percent: 23.833333333333332
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -714.0
    agent-3: -1418.0
    agent-4: -518.0
  policy_reward_mean:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -714.0
    agent-3: -1418.0
    agent-4: -518.0
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -714.0
    agent-3: -1418.0
    agent-4: -518.0
  sampler_perf:
    mean_action_processing_ms: 0.18768319518411372
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.1010416700160401
    mean_inference_ms: 20.513967422528413
    mean_raw_obs_processing_ms: 2.478747130113173
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed:
      - 4
      agent-0-beam_fired:
      - 145
      agent-0-beam_hit:
      - 24
      agent-0-cleaning:
      - 103.0
      agent-1-apples_consumed:
      - 0
      agent-1-beam_fired:
      - 83
      agent-1-beam_hit:
      - 29
      agent-1-cleaning:
      - 105.0
      agent-2-apples_consumed:
      - 2
      agent-2-beam_fired:
      - 115
      agent-2-beam_hit:
      - 11
      agent-2-cleaning:
      - 121.0
      agent-3-apples_consumed:
      - 0
      agent-3-beam_fired:
      - 115
      agent-3-beam_hit:
      - 23
      agent-3-cleaning:
      - 104.0
      agent-4-apples_consumed:
      - 5
      agent-4-beam_fired:
      - 72
      agent-4-beam_hit:
      - 8
      agent-4-cleaning:
      - 96.0
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -5779.0
    episode_reward_mean: -5779.0
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths:
      - 1000
      episode_reward:
      - -5779.0
      policy_agent-0_reward:
      - -1494.0
      policy_agent-1_reward:
      - -1635.0
      policy_agent-2_reward:
      - -714.0
      policy_agent-3_reward:
      - -1418.0
      policy_agent-4_reward:
      - -518.0
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -714.0
      agent-3: -1418.0
      agent-4: -518.0
    policy_reward_mean:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -714.0
      agent-3: -1418.0
      agent-4: -518.0
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -714.0
      agent-3: -1418.0
      agent-4: -518.0
    sampler_perf:
      mean_action_processing_ms: 0.18768319518411372
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.1010416700160401
      mean_inference_ms: 20.513967422528413
      mean_raw_obs_processing_ms: 2.478747130113173
  time_since_restore: 30.588918209075928
  time_this_iter_s: 5.133208990097046
  time_total_s: 30.588918209075928
  timers:
    apply_grad_throughput: 2184.618
    apply_grad_time_ms: 22.887
    grad_wait_time_ms: 0.233
    synch_weights_time_ms: 6.121
    training_iteration_time_ms: 0.248
  timestamp: 1661790017
  timesteps_since_restore: 0
  timesteps_total: 1270
  training_iteration: 6
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:20:22 (running for 00:01:06.10)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |      6 |          30.5889 | 1270 |    -5779 |                      0 |                -5779 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 7450
  counters:
    num_agent_steps_sampled: 7450
    num_agent_steps_trained: 7450
    num_env_steps_sampled: 1490
    num_env_steps_trained: 1490
  custom_metrics:
    agent-0-apples_consumed:
    - 4
    agent-0-beam_fired:
    - 145
    agent-0-beam_hit:
    - 24
    agent-0-cleaning:
    - 103.0
    agent-1-apples_consumed:
    - 0
    agent-1-beam_fired:
    - 83
    agent-1-beam_hit:
    - 29
    agent-1-cleaning:
    - 105.0
    agent-2-apples_consumed:
    - 2
    agent-2-beam_fired:
    - 115
    agent-2-beam_hit:
    - 11
    agent-2-cleaning:
    - 121.0
    agent-3-apples_consumed:
    - 0
    agent-3-beam_fired:
    - 115
    agent-3-beam_hit:
    - 23
    agent-3-cleaning:
    - 104.0
    agent-4-apples_consumed:
    - 5
    agent-4-beam_fired:
    - 72
    agent-4-beam_hit:
    - 8
    agent-4-cleaning:
    - 96.0
  date: 2022-08-29_17-20-22
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -5779.0
  episode_reward_mean: -5779.0
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 1
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 7450
    num_agent_steps_trained: 7450
    num_env_steps_sampled: 1490
    num_env_steps_trained: 1490
  iterations_since_restore: 7
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 7450
  num_agent_steps_trained: 7450
  num_env_steps_sampled: 1490
  num_env_steps_sampled_this_iter: 220
  num_env_steps_trained: 1490
  num_env_steps_trained_this_iter: 220
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 220
  perf:
    cpu_util_percent: 10.933333333333332
    gpu_util_percent0: 0.0
    ram_util_percent: 23.833333333333332
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -714.0
    agent-3: -1418.0
    agent-4: -518.0
  policy_reward_mean:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -714.0
    agent-3: -1418.0
    agent-4: -518.0
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -714.0
    agent-3: -1418.0
    agent-4: -518.0
  sampler_perf:
    mean_action_processing_ms: 0.18768319518411372
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.1010416700160401
    mean_inference_ms: 20.513967422528413
    mean_raw_obs_processing_ms: 2.478747130113173
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed:
      - 4
      agent-0-beam_fired:
      - 145
      agent-0-beam_hit:
      - 24
      agent-0-cleaning:
      - 103.0
      agent-1-apples_consumed:
      - 0
      agent-1-beam_fired:
      - 83
      agent-1-beam_hit:
      - 29
      agent-1-cleaning:
      - 105.0
      agent-2-apples_consumed:
      - 2
      agent-2-beam_fired:
      - 115
      agent-2-beam_hit:
      - 11
      agent-2-cleaning:
      - 121.0
      agent-3-apples_consumed:
      - 0
      agent-3-beam_fired:
      - 115
      agent-3-beam_hit:
      - 23
      agent-3-cleaning:
      - 104.0
      agent-4-apples_consumed:
      - 5
      agent-4-beam_fired:
      - 72
      agent-4-beam_hit:
      - 8
      agent-4-cleaning:
      - 96.0
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -5779.0
    episode_reward_mean: -5779.0
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths:
      - 1000
      episode_reward:
      - -5779.0
      policy_agent-0_reward:
      - -1494.0
      policy_agent-1_reward:
      - -1635.0
      policy_agent-2_reward:
      - -714.0
      policy_agent-3_reward:
      - -1418.0
      policy_agent-4_reward:
      - -518.0
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -714.0
      agent-3: -1418.0
      agent-4: -518.0
    policy_reward_mean:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -714.0
      agent-3: -1418.0
      agent-4: -518.0
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -714.0
      agent-3: -1418.0
      agent-4: -518.0
    sampler_perf:
      mean_action_processing_ms: 0.18768319518411372
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.1010416700160401
      mean_inference_ms: 20.513967422528413
      mean_raw_obs_processing_ms: 2.478747130113173
  time_since_restore: 35.69171953201294
  time_this_iter_s: 5.102801322937012
  time_total_s: 35.69171953201294
  timers:
    apply_grad_throughput: 2363.3
    apply_grad_time_ms: 21.157
    grad_wait_time_ms: 0.174
    synch_weights_time_ms: 5.742
    training_iteration_time_ms: 0.188
  timestamp: 1661790022
  timesteps_since_restore: 0
  timesteps_total: 1490
  training_iteration: 7
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:20:28 (running for 00:01:11.22)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |      7 |          35.6917 | 1490 |    -5779 |                      0 |                -5779 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 8550
  counters:
    num_agent_steps_sampled: 8550
    num_agent_steps_trained: 8550
    num_env_steps_sampled: 1710
    num_env_steps_trained: 1710
  custom_metrics:
    agent-0-apples_consumed:
    - 4
    agent-0-beam_fired:
    - 145
    agent-0-beam_hit:
    - 24
    agent-0-cleaning:
    - 103.0
    agent-1-apples_consumed:
    - 0
    agent-1-beam_fired:
    - 83
    agent-1-beam_hit:
    - 29
    agent-1-cleaning:
    - 105.0
    agent-2-apples_consumed:
    - 2
    agent-2-beam_fired:
    - 115
    agent-2-beam_hit:
    - 11
    agent-2-cleaning:
    - 121.0
    agent-3-apples_consumed:
    - 0
    agent-3-beam_fired:
    - 115
    agent-3-beam_hit:
    - 23
    agent-3-cleaning:
    - 104.0
    agent-4-apples_consumed:
    - 5
    agent-4-beam_fired:
    - 72
    agent-4-beam_hit:
    - 8
    agent-4-cleaning:
    - 96.0
  date: 2022-08-29_17-20-28
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -5779.0
  episode_reward_mean: -5779.0
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 1
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 8550
    num_agent_steps_trained: 8550
    num_env_steps_sampled: 1710
    num_env_steps_trained: 1710
  iterations_since_restore: 8
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 8550
  num_agent_steps_trained: 8550
  num_env_steps_sampled: 1710
  num_env_steps_sampled_this_iter: 220
  num_env_steps_trained: 1710
  num_env_steps_trained_this_iter: 220
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 220
  perf:
    cpu_util_percent: 10.800000000000002
    gpu_util_percent0: 0.0
    ram_util_percent: 23.899999999999995
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -714.0
    agent-3: -1418.0
    agent-4: -518.0
  policy_reward_mean:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -714.0
    agent-3: -1418.0
    agent-4: -518.0
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -714.0
    agent-3: -1418.0
    agent-4: -518.0
  sampler_perf:
    mean_action_processing_ms: 0.18768319518411372
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.1010416700160401
    mean_inference_ms: 20.513967422528413
    mean_raw_obs_processing_ms: 2.478747130113173
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed:
      - 4
      agent-0-beam_fired:
      - 145
      agent-0-beam_hit:
      - 24
      agent-0-cleaning:
      - 103.0
      agent-1-apples_consumed:
      - 0
      agent-1-beam_fired:
      - 83
      agent-1-beam_hit:
      - 29
      agent-1-cleaning:
      - 105.0
      agent-2-apples_consumed:
      - 2
      agent-2-beam_fired:
      - 115
      agent-2-beam_hit:
      - 11
      agent-2-cleaning:
      - 121.0
      agent-3-apples_consumed:
      - 0
      agent-3-beam_fired:
      - 115
      agent-3-beam_hit:
      - 23
      agent-3-cleaning:
      - 104.0
      agent-4-apples_consumed:
      - 5
      agent-4-beam_fired:
      - 72
      agent-4-beam_hit:
      - 8
      agent-4-cleaning:
      - 96.0
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -5779.0
    episode_reward_mean: -5779.0
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths:
      - 1000
      episode_reward:
      - -5779.0
      policy_agent-0_reward:
      - -1494.0
      policy_agent-1_reward:
      - -1635.0
      policy_agent-2_reward:
      - -714.0
      policy_agent-3_reward:
      - -1418.0
      policy_agent-4_reward:
      - -518.0
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -714.0
      agent-3: -1418.0
      agent-4: -518.0
    policy_reward_mean:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -714.0
      agent-3: -1418.0
      agent-4: -518.0
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -714.0
      agent-3: -1418.0
      agent-4: -518.0
    sampler_perf:
      mean_action_processing_ms: 0.18768319518411372
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.1010416700160401
      mean_inference_ms: 20.513967422528413
      mean_raw_obs_processing_ms: 2.478747130113173
  time_since_restore: 40.772756814956665
  time_this_iter_s: 5.081037282943726
  time_total_s: 40.772756814956665
  timers:
    apply_grad_throughput: 2462.102
    apply_grad_time_ms: 20.308
    grad_wait_time_ms: 0.148
    synch_weights_time_ms: 5.703
    training_iteration_time_ms: 0.16
  timestamp: 1661790028
  timesteps_since_restore: 0
  timesteps_total: 1710
  training_iteration: 8
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:20:33 (running for 00:01:16.46)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |      8 |          40.7728 | 1710 |    -5779 |                      0 |                -5779 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 9650
  counters:
    num_agent_steps_sampled: 9650
    num_agent_steps_trained: 9650
    num_env_steps_sampled: 1930
    num_env_steps_trained: 1930
  custom_metrics:
    agent-0-apples_consumed:
    - 4
    agent-0-beam_fired:
    - 145
    agent-0-beam_hit:
    - 24
    agent-0-cleaning:
    - 103.0
    agent-1-apples_consumed:
    - 0
    agent-1-beam_fired:
    - 83
    agent-1-beam_hit:
    - 29
    agent-1-cleaning:
    - 105.0
    agent-2-apples_consumed:
    - 2
    agent-2-beam_fired:
    - 115
    agent-2-beam_hit:
    - 11
    agent-2-cleaning:
    - 121.0
    agent-3-apples_consumed:
    - 0
    agent-3-beam_fired:
    - 115
    agent-3-beam_hit:
    - 23
    agent-3-cleaning:
    - 104.0
    agent-4-apples_consumed:
    - 5
    agent-4-beam_fired:
    - 72
    agent-4-beam_hit:
    - 8
    agent-4-cleaning:
    - 96.0
  date: 2022-08-29_17-20-33
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -5779.0
  episode_reward_mean: -5779.0
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 1
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 9650
    num_agent_steps_trained: 9650
    num_env_steps_sampled: 1930
    num_env_steps_trained: 1930
  iterations_since_restore: 9
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 9650
  num_agent_steps_trained: 9650
  num_env_steps_sampled: 1930
  num_env_steps_sampled_this_iter: 220
  num_env_steps_trained: 1930
  num_env_steps_trained_this_iter: 220
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 220
  perf:
    cpu_util_percent: 10.766666666666666
    gpu_util_percent0: 0.0
    ram_util_percent: 23.899999999999995
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -714.0
    agent-3: -1418.0
    agent-4: -518.0
  policy_reward_mean:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -714.0
    agent-3: -1418.0
    agent-4: -518.0
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -714.0
    agent-3: -1418.0
    agent-4: -518.0
  sampler_perf:
    mean_action_processing_ms: 0.18768319518411372
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.1010416700160401
    mean_inference_ms: 20.513967422528413
    mean_raw_obs_processing_ms: 2.478747130113173
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed:
      - 4
      agent-0-beam_fired:
      - 145
      agent-0-beam_hit:
      - 24
      agent-0-cleaning:
      - 103.0
      agent-1-apples_consumed:
      - 0
      agent-1-beam_fired:
      - 83
      agent-1-beam_hit:
      - 29
      agent-1-cleaning:
      - 105.0
      agent-2-apples_consumed:
      - 2
      agent-2-beam_fired:
      - 115
      agent-2-beam_hit:
      - 11
      agent-2-cleaning:
      - 121.0
      agent-3-apples_consumed:
      - 0
      agent-3-beam_fired:
      - 115
      agent-3-beam_hit:
      - 23
      agent-3-cleaning:
      - 104.0
      agent-4-apples_consumed:
      - 5
      agent-4-beam_fired:
      - 72
      agent-4-beam_hit:
      - 8
      agent-4-cleaning:
      - 96.0
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -5779.0
    episode_reward_mean: -5779.0
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths:
      - 1000
      episode_reward:
      - -5779.0
      policy_agent-0_reward:
      - -1494.0
      policy_agent-1_reward:
      - -1635.0
      policy_agent-2_reward:
      - -714.0
      policy_agent-3_reward:
      - -1418.0
      policy_agent-4_reward:
      - -518.0
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -714.0
      agent-3: -1418.0
      agent-4: -518.0
    policy_reward_mean:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -714.0
      agent-3: -1418.0
      agent-4: -518.0
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -714.0
      agent-3: -1418.0
      agent-4: -518.0
    sampler_perf:
      mean_action_processing_ms: 0.18768319518411372
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.1010416700160401
      mean_inference_ms: 20.513967422528413
      mean_raw_obs_processing_ms: 2.478747130113173
  time_since_restore: 45.83792567253113
  time_this_iter_s: 5.065168857574463
  time_total_s: 45.83792567253113
  timers:
    apply_grad_throughput: 2530.412
    apply_grad_time_ms: 19.76
    grad_wait_time_ms: 0.176
    synch_weights_time_ms: 5.788
    training_iteration_time_ms: 0.194
  timestamp: 1661790033
  timesteps_since_restore: 0
  timesteps_total: 1930
  training_iteration: 9
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

(RolloutWorker pid=18610) check agent ids ['agent-0', 'agent-1', 'agent-2', 'agent-3', 'agent-4']
(RolloutWorker pid=18610) checking again ['agent-0', 'agent-1', 'agent-2', 'agent-3', 'agent-4']
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) here we are agent-4 [1, 4, 1, 1, 5, 6, 3, 3, 5, 0, 5, 3, 3, 6, 3, 8, 5, 1, 0, 3, 5, 5, 8, 8, 8, 3, 2, 0, 0, 1, 8, 3, 5, 1, 2, 8, 3, 0, 6, 5, 3, 7, 5, 8, 1, 4, 2, 5, 6, 2, 1, 1, 6, 5, 1, 0, 2, 6, 4, 3, 0, 0, 1, 2, 1, 3, 1, 8, 3, 1, 5, 2, 8, 6, 6, 6, 4, 7, 2, 1, 1, 5, 4, 5, 6, 3, 8, 8, 4, 4, 1, 3, 6, 8, 1, 1, 1, 8, 0, 1, 8, 5, 4, 7, 5, 5, 4, 5, 3, 7, 0, 3, 5, 7, 5, 2, 6, 8, 8, 5, 3, 4, 3, 6, 3, 1, 3, 8, 3, 7, 3, 5, 6, 4, 5, 8, 3, 8, 2, 0, 2, 0, 4, 4, 7, 4, 0, 0, 6, 2, 4, 1, 7, 1, 3, 5, 6, 3, 2, 1, 4, 8, 8, 5, 3, 6, 5, 5, 1, 3, 5, 8, 3, 0, 3, 8, 3, 6, 5, 0, 3, 0, 1, 8, 8, 2, 2, 7, 5, 6, 4, 5, 6, 6, 8, 6, 6, 1, 8, 4, 8, 3, 3, 4, 3, 8, 3, 3, 5, 4, 5, 8, 0, 3, 3, 6, 6, 5, 6, 3, 8, 3, 6, 4, 0, 2, 0, 4, 2, 1, 7, 6, 8, 7, 5, 5, 3, 5, 3, 6, 5, 3, 7, 6, 6, 6, 5, 3, 2, 4, 2, 2, 5, 2, 3, 3, 4, 6, 6, 1, 1, 2, 8, 2, 3, 8, 4, 2, 3, 0, 6, 5, 3, 3, 4, 4, 1, 6, 4, 1, 0, 0, 5, 7, 5, 0, 8, 0, 8, 0, 1, 3, 5, 6, 1, 8, 6, 5, 2, 1, 6, 6, 4, 5, 1, 1, 5, 3, 1, 0, 4, 1, 6, 4, 7, 2, 1, 3, 7, 7, 6, 2, 8, 8, 6, 6, 7, 2, 0, 8, 6, 0, 8, 5, 4, 2, 3, 1, 6, 6, 4, 5, 5, 6, 3, 7, 1, 3, 3, 8, 1, 4, 1, 8, 6, 6, 6, 8, 8, 1, 3, 1, 1, 8, 6, 3, 2, 3, 3, 1, 7, 3, 3, 1, 5, 1, 0, 7, 7, 1, 7, 2, 6, 7, 7, 3, 3, 7, 5, 5, 7, 2, 3, 4, 5, 6, 3, 3, 1, 0, 8, 1, 1, 4, 6, 4, 0, 6, 6, 2, 3, 1, 8, 2, 8, 4, 1, 4, 4, 3, 7, 8, 4, 0, 6, 8, 3, 4, 1, 4, 4, 5, 3, 1, 6, 0, 8, 5, 8, 5, 6, 4, 8, 8, 6, 6, 7, 5, 7, 6, 5, 3, 8, 4, 5, 5, 1, 4, 5, 2, 5, 5, 3, 6, 5, 0, 1, 3, 4, 4, 0, 1, 2, 0, 3, 2, 3, 4, 2, 3, 0, 8, 3, 2, 0, 1, 4, 3, 2, 5, 3, 1, 6, 8, 7, 2, 5, 5, 8, 1, 8, 6, 4, 1, 3, 5, 0, 3, 7, 0, 1, 3, 0, 1, 5, 1, 5, 8, 8, 8, 6, 1, 1, 8, 6, 4, 0, 4, 4, 3, 0, 8, 0, 3, 7, 1, 8, 5, 4, 6, 7, 6, 7, 1, 8, 6, 6, 3, 1, 2, 3, 4, 8, 2, 4, 1, 1, 3, 8, 3, 5, 7, 4, 6, 1, 1, 3, 6, 5, 8, 1, 3, 7, 5, 6, 6, 2, 0, 2, 6, 2, 5, 4, 3, 3, 1, 8, 3, 1, 2, 6, 8, 5, 3, 5, 1, 2, 4, 2, 3, 1, 8, 0, 1, 2, 3, 3, 6, 2, 2, 4, 5, 5, 1, 4, 4, 5, 6, 0, 3, 6, 8, 8, 3, 3, 1, 8, 3, 0, 6, 6, 1, 0, 6, 0, 5, 5, 0, 0, 4, 0, 4, 7, 5, 3, 6, 2, 3, 4, 0, 5, 0, 7, 6, 0, 1, 7, 6, 5, 1, 0, 0, 3, 3, 4, 8, 5, 6, 1, 6, 3, 4, 3, 6, 0, 6, 3, 4, 8, 4, 5, 0, 6, 6, 2, 3, 7, 1, 6, 8, 3, 8, 1, 6, 6, 6, 1, 5, 5, 6, 1, 6, 1, 1, 8, 4, 7, 1, 3, 7, 1, 2, 8, 6, 1, 7, 4, 8, 3, 3, 6, 5, 4, 7, 3, 6, 1, 4, 0, 8, 0, 5, 3, 3, 1, 6, 8, 0, 5, 7, 6, 6, 2, 0, 5, 5, 1, 2, 1, 2, 1, 6, 5, 5, 4, 6, 0, 7, 5, 3, 8, 6, 3, 3, 0, 1, 6, 5, 8, 8, 0, 6, 0, 3, 3, 3, 5, 1, 1, 3, 8, 0, 8, 6, 3, 5, 3, 7, 5, 2, 6, 0, 3, 8, 5, 8, 7, 7, 3, 0, 5, 6, 8, 6, 5, 0, 5, 8, 8, 5, 6, 1, 4, 2, 5, 6, 3, 6, 3, 8, 8, 8, 1, 6, 2, 5, 3, 5, 4, 4, 6, 0, 4, 6, 5, 0, 3, 2, 0, 1, 6, 5, 2, 5, 8, 5, 7, 8, 0, 5, 3, 4, 4, 1, 3, 2, 1, 5, 2, 5, 6, 7, 0, 3, 8, 8, 1, 5, 2, 3, 5, 3, 1, 5, 1, 3, 6, 1, 4, 2, 3, 1, 8, 3, 6, 5, 8, 1, 3, 5, 3, 8, 8, 8, 8, 3, 0, 6, 0, 5, 1, 6, 5, 5, 5, 2, 4, 3, 0, 5, 2, 4, 0, 1, 1, 2, 3, 3, 0, 4, 3, 4, 7, 8, 3, 4, 8, 1, 4, 1, 4, 8, 3, 8, 4, 2, 3, 1, 5, 3, 4, 5, 8, 3, 1, 4, 2, 3, 7, 5, 6, 6, 4, 5, 6, 7, 7, 8, 5, 6, 1, 8, 6, 3, 0, 4, 8, 8, 1, 1, 4, 8, 2, 3, 8, 8, 3, 2, 8, 6, 4, 4, 8, 3, 5, 4, 0, 3, 4, 3, 1, 3, 3, 3, 2, 0, 7, 2, 3, 3]
(RolloutWorker pid=18610) and here {'agent-0-beam_fired': 98, 'agent-0-cleaning': 105.0, 'agent-0-beam_hit': 4, 'agent-0-apples_consumed': 0, 'agent-1-beam_fired': 63, 'agent-1-cleaning': 87.0, 'agent-1-beam_hit': 5, 'agent-1-apples_consumed': 9, 'agent-2-beam_fired': 81, 'agent-2-cleaning': 135.0, 'agent-2-beam_hit': 13, 'agent-2-apples_consumed': 4, 'agent-3-beam_fired': 90, 'agent-3-cleaning': 85.0, 'agent-3-beam_hit': 11, 'agent-3-apples_consumed': 15, 'agent-4-beam_fired': 55, 'agent-4-cleaning': 122.0, 'agent-4-beam_hit': 14, 'agent-4-apples_consumed': 14}
(RolloutWorker pid=18610) episode 1864629672 (env-idx=0) started.
(RolloutWorker pid=18610) agent id  agent-0
(RolloutWorker pid=18610) agent id  agent-1
(RolloutWorker pid=18610) agent id  agent-2
(RolloutWorker pid=18610) agent id  agent-3
(RolloutWorker pid=18610) agent id  agent-4
== Status ==
Current time: 2022-08-29 17:20:38 (running for 00:01:21.55)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |      9 |          45.8379 | 1930 |    -5779 |                      0 |                -5779 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 10750
  counters:
    num_agent_steps_sampled: 10750
    num_agent_steps_trained: 10750
    num_env_steps_sampled: 2150
    num_env_steps_trained: 2150
  custom_metrics:
    agent-0-apples_consumed:
    - 4
    - 0
    agent-0-beam_fired:
    - 145
    - 98
    agent-0-beam_hit:
    - 24
    - 4
    agent-0-cleaning:
    - 103.0
    - 105.0
    agent-1-apples_consumed:
    - 0
    - 9
    agent-1-beam_fired:
    - 83
    - 63
    agent-1-beam_hit:
    - 29
    - 5
    agent-1-cleaning:
    - 105.0
    - 87.0
    agent-2-apples_consumed:
    - 2
    - 4
    agent-2-beam_fired:
    - 115
    - 81
    agent-2-beam_hit:
    - 11
    - 13
    agent-2-cleaning:
    - 121.0
    - 135.0
    agent-3-apples_consumed:
    - 0
    - 15
    agent-3-beam_fired:
    - 115
    - 90
    agent-3-beam_hit:
    - 23
    - 11
    agent-3-cleaning:
    - 104.0
    - 85.0
    agent-4-apples_consumed:
    - 5
    - 14
    agent-4-beam_fired:
    - 72
    - 55
    agent-4-beam_hit:
    - 8
    - 14
    agent-4-cleaning:
    - 96.0
    - 122.0
  date: 2022-08-29_17-20-38
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -3101.0
  episode_reward_mean: -4440.0
  episode_reward_min: -5779.0
  episodes_this_iter: 1
  episodes_total: 2
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 10750
    num_agent_steps_trained: 10750
    num_env_steps_sampled: 2150
    num_env_steps_trained: 2150
  iterations_since_restore: 10
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 10750
  num_agent_steps_trained: 10750
  num_env_steps_sampled: 2150
  num_env_steps_sampled_this_iter: 220
  num_env_steps_trained: 2150
  num_env_steps_trained_this_iter: 220
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 220
  perf:
    cpu_util_percent: 11.566666666666668
    gpu_util_percent0: 0.0
    ram_util_percent: 23.8
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -400.0
    agent-1: -304.0
    agent-2: -714.0
    agent-3: -776.0
    agent-4: -518.0
  policy_reward_mean:
    agent-0: -947.0
    agent-1: -969.5
    agent-2: -771.5
    agent-3: -1097.0
    agent-4: -655.0
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.18516533770072144
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.0867401470471383
    mean_inference_ms: 20.413074247483166
    mean_raw_obs_processing_ms: 2.4307977201485835
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed:
      - 4
      - 0
      agent-0-beam_fired:
      - 145
      - 98
      agent-0-beam_hit:
      - 24
      - 4
      agent-0-cleaning:
      - 103.0
      - 105.0
      agent-1-apples_consumed:
      - 0
      - 9
      agent-1-beam_fired:
      - 83
      - 63
      agent-1-beam_hit:
      - 29
      - 5
      agent-1-cleaning:
      - 105.0
      - 87.0
      agent-2-apples_consumed:
      - 2
      - 4
      agent-2-beam_fired:
      - 115
      - 81
      agent-2-beam_hit:
      - 11
      - 13
      agent-2-cleaning:
      - 121.0
      - 135.0
      agent-3-apples_consumed:
      - 0
      - 15
      agent-3-beam_fired:
      - 115
      - 90
      agent-3-beam_hit:
      - 23
      - 11
      agent-3-cleaning:
      - 104.0
      - 85.0
      agent-4-apples_consumed:
      - 5
      - 14
      agent-4-beam_fired:
      - 72
      - 55
      agent-4-beam_hit:
      - 8
      - 14
      agent-4-cleaning:
      - 96.0
      - 122.0
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -3101.0
    episode_reward_mean: -4440.0
    episode_reward_min: -5779.0
    episodes_this_iter: 1
    hist_stats:
      episode_lengths:
      - 1000
      - 1000
      episode_reward:
      - -5779.0
      - -3101.0
      policy_agent-0_reward:
      - -1494.0
      - -400.0
      policy_agent-1_reward:
      - -1635.0
      - -304.0
      policy_agent-2_reward:
      - -714.0
      - -829.0
      policy_agent-3_reward:
      - -1418.0
      - -776.0
      policy_agent-4_reward:
      - -518.0
      - -792.0
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -400.0
      agent-1: -304.0
      agent-2: -714.0
      agent-3: -776.0
      agent-4: -518.0
    policy_reward_mean:
      agent-0: -947.0
      agent-1: -969.5
      agent-2: -771.5
      agent-3: -1097.0
      agent-4: -655.0
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.18516533770072144
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.0867401470471383
      mean_inference_ms: 20.413074247483166
      mean_raw_obs_processing_ms: 2.4307977201485835
  time_since_restore: 50.97005748748779
  time_this_iter_s: 5.132131814956665
  time_total_s: 50.97005748748779
  timers:
    apply_grad_throughput: 2466.434
    apply_grad_time_ms: 20.272
    grad_wait_time_ms: 0.163
    synch_weights_time_ms: 5.498
    training_iteration_time_ms: 0.176
  timestamp: 1661790038
  timesteps_since_restore: 0
  timesteps_total: 2150
  training_iteration: 10
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:20:43 (running for 00:01:26.89)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |     10 |          50.9701 | 2150 |    -4440 |                      0 |                -3101 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 11850
  counters:
    num_agent_steps_sampled: 11850
    num_agent_steps_trained: 11850
    num_env_steps_sampled: 2370
    num_env_steps_trained: 2370
  custom_metrics:
    agent-0-apples_consumed:
    - 4
    - 0
    agent-0-beam_fired:
    - 145
    - 98
    agent-0-beam_hit:
    - 24
    - 4
    agent-0-cleaning:
    - 103.0
    - 105.0
    agent-1-apples_consumed:
    - 0
    - 9
    agent-1-beam_fired:
    - 83
    - 63
    agent-1-beam_hit:
    - 29
    - 5
    agent-1-cleaning:
    - 105.0
    - 87.0
    agent-2-apples_consumed:
    - 2
    - 4
    agent-2-beam_fired:
    - 115
    - 81
    agent-2-beam_hit:
    - 11
    - 13
    agent-2-cleaning:
    - 121.0
    - 135.0
    agent-3-apples_consumed:
    - 0
    - 15
    agent-3-beam_fired:
    - 115
    - 90
    agent-3-beam_hit:
    - 23
    - 11
    agent-3-cleaning:
    - 104.0
    - 85.0
    agent-4-apples_consumed:
    - 5
    - 14
    agent-4-beam_fired:
    - 72
    - 55
    agent-4-beam_hit:
    - 8
    - 14
    agent-4-cleaning:
    - 96.0
    - 122.0
  date: 2022-08-29_17-20-43
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -3101.0
  episode_reward_mean: -4440.0
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 2
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 11850
    num_agent_steps_trained: 11850
    num_env_steps_sampled: 2370
    num_env_steps_trained: 2370
  iterations_since_restore: 11
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 11850
  num_agent_steps_trained: 11850
  num_env_steps_sampled: 2370
  num_env_steps_sampled_this_iter: 220
  num_env_steps_trained: 2370
  num_env_steps_trained_this_iter: 220
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 220
  perf:
    cpu_util_percent: 10.833333333333334
    gpu_util_percent0: 0.0
    ram_util_percent: 23.899999999999995
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -400.0
    agent-1: -304.0
    agent-2: -714.0
    agent-3: -776.0
    agent-4: -518.0
  policy_reward_mean:
    agent-0: -947.0
    agent-1: -969.5
    agent-2: -771.5
    agent-3: -1097.0
    agent-4: -655.0
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.18516533770072144
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.0867401470471383
    mean_inference_ms: 20.413074247483166
    mean_raw_obs_processing_ms: 2.4307977201485835
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed:
      - 4
      - 0
      agent-0-beam_fired:
      - 145
      - 98
      agent-0-beam_hit:
      - 24
      - 4
      agent-0-cleaning:
      - 103.0
      - 105.0
      agent-1-apples_consumed:
      - 0
      - 9
      agent-1-beam_fired:
      - 83
      - 63
      agent-1-beam_hit:
      - 29
      - 5
      agent-1-cleaning:
      - 105.0
      - 87.0
      agent-2-apples_consumed:
      - 2
      - 4
      agent-2-beam_fired:
      - 115
      - 81
      agent-2-beam_hit:
      - 11
      - 13
      agent-2-cleaning:
      - 121.0
      - 135.0
      agent-3-apples_consumed:
      - 0
      - 15
      agent-3-beam_fired:
      - 115
      - 90
      agent-3-beam_hit:
      - 23
      - 11
      agent-3-cleaning:
      - 104.0
      - 85.0
      agent-4-apples_consumed:
      - 5
      - 14
      agent-4-beam_fired:
      - 72
      - 55
      agent-4-beam_hit:
      - 8
      - 14
      agent-4-cleaning:
      - 96.0
      - 122.0
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -3101.0
    episode_reward_mean: -4440.0
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths:
      - 1000
      - 1000
      episode_reward:
      - -5779.0
      - -3101.0
      policy_agent-0_reward:
      - -1494.0
      - -400.0
      policy_agent-1_reward:
      - -1635.0
      - -304.0
      policy_agent-2_reward:
      - -714.0
      - -829.0
      policy_agent-3_reward:
      - -1418.0
      - -776.0
      policy_agent-4_reward:
      - -518.0
      - -792.0
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -400.0
      agent-1: -304.0
      agent-2: -714.0
      agent-3: -776.0
      agent-4: -518.0
    policy_reward_mean:
      agent-0: -947.0
      agent-1: -969.5
      agent-2: -771.5
      agent-3: -1097.0
      agent-4: -655.0
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.18516533770072144
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.0867401470471383
      mean_inference_ms: 20.413074247483166
      mean_raw_obs_processing_ms: 2.4307977201485835
  time_since_restore: 56.115713119506836
  time_this_iter_s: 5.145655632019043
  time_total_s: 56.115713119506836
  timers:
    apply_grad_throughput: 2424.823
    apply_grad_time_ms: 20.62
    grad_wait_time_ms: 0.143
    synch_weights_time_ms: 5.537
    training_iteration_time_ms: 0.156
  timestamp: 1661790043
  timesteps_since_restore: 0
  timesteps_total: 2370
  training_iteration: 11
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:20:48 (running for 00:01:32.07)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |     11 |          56.1157 | 2370 |    -4440 |                      0 |                -3101 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 12950
  counters:
    num_agent_steps_sampled: 12950
    num_agent_steps_trained: 12950
    num_env_steps_sampled: 2590
    num_env_steps_trained: 2590
  custom_metrics:
    agent-0-apples_consumed:
    - 4
    - 0
    agent-0-beam_fired:
    - 145
    - 98
    agent-0-beam_hit:
    - 24
    - 4
    agent-0-cleaning:
    - 103.0
    - 105.0
    agent-1-apples_consumed:
    - 0
    - 9
    agent-1-beam_fired:
    - 83
    - 63
    agent-1-beam_hit:
    - 29
    - 5
    agent-1-cleaning:
    - 105.0
    - 87.0
    agent-2-apples_consumed:
    - 2
    - 4
    agent-2-beam_fired:
    - 115
    - 81
    agent-2-beam_hit:
    - 11
    - 13
    agent-2-cleaning:
    - 121.0
    - 135.0
    agent-3-apples_consumed:
    - 0
    - 15
    agent-3-beam_fired:
    - 115
    - 90
    agent-3-beam_hit:
    - 23
    - 11
    agent-3-cleaning:
    - 104.0
    - 85.0
    agent-4-apples_consumed:
    - 5
    - 14
    agent-4-beam_fired:
    - 72
    - 55
    agent-4-beam_hit:
    - 8
    - 14
    agent-4-cleaning:
    - 96.0
    - 122.0
  date: 2022-08-29_17-20-49
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -3101.0
  episode_reward_mean: -4440.0
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 2
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 12950
    num_agent_steps_trained: 12950
    num_env_steps_sampled: 2590
    num_env_steps_trained: 2590
  iterations_since_restore: 12
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 12950
  num_agent_steps_trained: 12950
  num_env_steps_sampled: 2590
  num_env_steps_sampled_this_iter: 220
  num_env_steps_trained: 2590
  num_env_steps_trained_this_iter: 220
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 220
  perf:
    cpu_util_percent: 11.166666666666666
    gpu_util_percent0: 0.0
    ram_util_percent: 23.866666666666664
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -400.0
    agent-1: -304.0
    agent-2: -714.0
    agent-3: -776.0
    agent-4: -518.0
  policy_reward_mean:
    agent-0: -947.0
    agent-1: -969.5
    agent-2: -771.5
    agent-3: -1097.0
    agent-4: -655.0
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.18516533770072144
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.0867401470471383
    mean_inference_ms: 20.413074247483166
    mean_raw_obs_processing_ms: 2.4307977201485835
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed:
      - 4
      - 0
      agent-0-beam_fired:
      - 145
      - 98
      agent-0-beam_hit:
      - 24
      - 4
      agent-0-cleaning:
      - 103.0
      - 105.0
      agent-1-apples_consumed:
      - 0
      - 9
      agent-1-beam_fired:
      - 83
      - 63
      agent-1-beam_hit:
      - 29
      - 5
      agent-1-cleaning:
      - 105.0
      - 87.0
      agent-2-apples_consumed:
      - 2
      - 4
      agent-2-beam_fired:
      - 115
      - 81
      agent-2-beam_hit:
      - 11
      - 13
      agent-2-cleaning:
      - 121.0
      - 135.0
      agent-3-apples_consumed:
      - 0
      - 15
      agent-3-beam_fired:
      - 115
      - 90
      agent-3-beam_hit:
      - 23
      - 11
      agent-3-cleaning:
      - 104.0
      - 85.0
      agent-4-apples_consumed:
      - 5
      - 14
      agent-4-beam_fired:
      - 72
      - 55
      agent-4-beam_hit:
      - 8
      - 14
      agent-4-cleaning:
      - 96.0
      - 122.0
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -3101.0
    episode_reward_mean: -4440.0
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths:
      - 1000
      - 1000
      episode_reward:
      - -5779.0
      - -3101.0
      policy_agent-0_reward:
      - -1494.0
      - -400.0
      policy_agent-1_reward:
      - -1635.0
      - -304.0
      policy_agent-2_reward:
      - -714.0
      - -829.0
      policy_agent-3_reward:
      - -1418.0
      - -776.0
      policy_agent-4_reward:
      - -518.0
      - -792.0
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -400.0
      agent-1: -304.0
      agent-2: -714.0
      agent-3: -776.0
      agent-4: -518.0
    policy_reward_mean:
      agent-0: -947.0
      agent-1: -969.5
      agent-2: -771.5
      agent-3: -1097.0
      agent-4: -655.0
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.18516533770072144
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.0867401470471383
      mean_inference_ms: 20.413074247483166
      mean_raw_obs_processing_ms: 2.4307977201485835
  time_since_restore: 61.32514452934265
  time_this_iter_s: 5.209431409835815
  time_total_s: 61.32514452934265
  timers:
    apply_grad_throughput: 2386.343
    apply_grad_time_ms: 20.953
    grad_wait_time_ms: 0.216
    synch_weights_time_ms: 5.986
    training_iteration_time_ms: 0.232
  timestamp: 1661790049
  timesteps_since_restore: 0
  timesteps_total: 2590
  training_iteration: 12
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:20:54 (running for 00:01:37.48)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |     12 |          61.3251 | 2590 |    -4440 |                      0 |                -3101 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 14050
  counters:
    num_agent_steps_sampled: 14050
    num_agent_steps_trained: 14050
    num_env_steps_sampled: 2810
    num_env_steps_trained: 2810
  custom_metrics:
    agent-0-apples_consumed:
    - 4
    - 0
    agent-0-beam_fired:
    - 145
    - 98
    agent-0-beam_hit:
    - 24
    - 4
    agent-0-cleaning:
    - 103.0
    - 105.0
    agent-1-apples_consumed:
    - 0
    - 9
    agent-1-beam_fired:
    - 83
    - 63
    agent-1-beam_hit:
    - 29
    - 5
    agent-1-cleaning:
    - 105.0
    - 87.0
    agent-2-apples_consumed:
    - 2
    - 4
    agent-2-beam_fired:
    - 115
    - 81
    agent-2-beam_hit:
    - 11
    - 13
    agent-2-cleaning:
    - 121.0
    - 135.0
    agent-3-apples_consumed:
    - 0
    - 15
    agent-3-beam_fired:
    - 115
    - 90
    agent-3-beam_hit:
    - 23
    - 11
    agent-3-cleaning:
    - 104.0
    - 85.0
    agent-4-apples_consumed:
    - 5
    - 14
    agent-4-beam_fired:
    - 72
    - 55
    agent-4-beam_hit:
    - 8
    - 14
    agent-4-cleaning:
    - 96.0
    - 122.0
  date: 2022-08-29_17-20-54
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -3101.0
  episode_reward_mean: -4440.0
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 2
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 14050
    num_agent_steps_trained: 14050
    num_env_steps_sampled: 2810
    num_env_steps_trained: 2810
  iterations_since_restore: 13
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 14050
  num_agent_steps_trained: 14050
  num_env_steps_sampled: 2810
  num_env_steps_sampled_this_iter: 220
  num_env_steps_trained: 2810
  num_env_steps_trained_this_iter: 220
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 220
  perf:
    cpu_util_percent: 10.633333333333333
    gpu_util_percent0: 0.0
    ram_util_percent: 23.866666666666664
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -400.0
    agent-1: -304.0
    agent-2: -714.0
    agent-3: -776.0
    agent-4: -518.0
  policy_reward_mean:
    agent-0: -947.0
    agent-1: -969.5
    agent-2: -771.5
    agent-3: -1097.0
    agent-4: -655.0
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.18516533770072144
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.0867401470471383
    mean_inference_ms: 20.413074247483166
    mean_raw_obs_processing_ms: 2.4307977201485835
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed:
      - 4
      - 0
      agent-0-beam_fired:
      - 145
      - 98
      agent-0-beam_hit:
      - 24
      - 4
      agent-0-cleaning:
      - 103.0
      - 105.0
      agent-1-apples_consumed:
      - 0
      - 9
      agent-1-beam_fired:
      - 83
      - 63
      agent-1-beam_hit:
      - 29
      - 5
      agent-1-cleaning:
      - 105.0
      - 87.0
      agent-2-apples_consumed:
      - 2
      - 4
      agent-2-beam_fired:
      - 115
      - 81
      agent-2-beam_hit:
      - 11
      - 13
      agent-2-cleaning:
      - 121.0
      - 135.0
      agent-3-apples_consumed:
      - 0
      - 15
      agent-3-beam_fired:
      - 115
      - 90
      agent-3-beam_hit:
      - 23
      - 11
      agent-3-cleaning:
      - 104.0
      - 85.0
      agent-4-apples_consumed:
      - 5
      - 14
      agent-4-beam_fired:
      - 72
      - 55
      agent-4-beam_hit:
      - 8
      - 14
      agent-4-cleaning:
      - 96.0
      - 122.0
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -3101.0
    episode_reward_mean: -4440.0
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths:
      - 1000
      - 1000
      episode_reward:
      - -5779.0
      - -3101.0
      policy_agent-0_reward:
      - -1494.0
      - -400.0
      policy_agent-1_reward:
      - -1635.0
      - -304.0
      policy_agent-2_reward:
      - -714.0
      - -829.0
      policy_agent-3_reward:
      - -1418.0
      - -776.0
      policy_agent-4_reward:
      - -518.0
      - -792.0
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -400.0
      agent-1: -304.0
      agent-2: -714.0
      agent-3: -776.0
      agent-4: -518.0
    policy_reward_mean:
      agent-0: -947.0
      agent-1: -969.5
      agent-2: -771.5
      agent-3: -1097.0
      agent-4: -655.0
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.18516533770072144
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.0867401470471383
      mean_inference_ms: 20.413074247483166
      mean_raw_obs_processing_ms: 2.4307977201485835
  time_since_restore: 66.44608926773071
  time_this_iter_s: 5.1209447383880615
  time_total_s: 66.44608926773071
  timers:
    apply_grad_throughput: 2407.688
    apply_grad_time_ms: 20.767
    grad_wait_time_ms: 0.2
    synch_weights_time_ms: 5.813
    training_iteration_time_ms: 0.215
  timestamp: 1661790054
  timesteps_since_restore: 0
  timesteps_total: 2810
  training_iteration: 13
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

(RolloutWorker pid=18610) check agent ids ['agent-0', 'agent-1', 'agent-2', 'agent-3', 'agent-4']
(RolloutWorker pid=18610) checking again ['agent-0', 'agent-1', 'agent-2', 'agent-3', 'agent-4']
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) here we are agent-4 [4, 1, 1, 1, 5, 7, 6, 1, 6, 3, 0, 3, 4, 2, 0, 6, 0, 3, 6, 5, 0, 3, 8, 8, 2, 6, 3, 5, 8, 1, 4, 3, 5, 4, 6, 6, 6, 8, 2, 2, 6, 0, 5, 1, 6, 4, 4, 6, 0, 6, 5, 6, 5, 8, 5, 4, 4, 6, 8, 2, 3, 6, 8, 0, 6, 2, 5, 4, 3, 4, 0, 2, 0, 1, 0, 8, 8, 5, 8, 7, 6, 6, 4, 4, 1, 6, 7, 4, 3, 5, 6, 1, 2, 5, 6, 3, 6, 0, 2, 8, 8, 8, 4, 5, 8, 3, 5, 5, 7, 1, 5, 8, 5, 3, 6, 2, 0, 2, 8, 0, 4, 5, 4, 6, 3, 0, 8, 6, 1, 4, 5, 1, 4, 5, 2, 3, 4, 5, 5, 0, 6, 5, 7, 6, 3, 3, 6, 1, 3, 0, 2, 1, 6, 3, 3, 6, 7, 4, 3, 5, 3, 5, 6, 3, 5, 6, 5, 2, 0, 3, 4, 6, 5, 7, 6, 5, 1, 0, 6, 6, 1, 8, 6, 4, 3, 1, 5, 3, 4, 3, 3, 5, 8, 2, 2, 6, 6, 1, 4, 8, 4, 3, 6, 1, 4, 5, 2, 2, 5, 6, 4, 1, 4, 8, 6, 2, 1, 6, 1, 6, 6, 1, 2, 8, 5, 1, 8, 5, 2, 6, 4, 6, 8, 6, 5, 5, 6, 4, 8, 4, 3, 6, 4, 5, 5, 1, 1, 5, 5, 5, 6, 5, 4, 4, 4, 4, 4, 6, 8, 5, 6, 3, 3, 0, 3, 1, 6, 3, 5, 6, 3, 3, 5, 1, 3, 0, 4, 0, 8, 0, 5, 3, 3, 3, 3, 4, 8, 4, 5, 8, 8, 4, 4, 7, 3, 3, 6, 5, 8, 8, 7, 1, 3, 6, 3, 3, 1, 1, 1, 4, 0, 0, 0, 0, 5, 4, 0, 0, 8, 6, 8, 7, 0, 1, 0, 5, 0, 1, 4, 2, 6, 4, 4, 4, 1, 1, 1, 7, 3, 5, 1, 3, 7, 0, 5, 2, 0, 7, 0, 5, 1, 6, 1, 3, 6, 6, 2, 7, 0, 8, 6, 3, 3, 6, 4, 5, 0, 0, 0, 6, 0, 5, 7, 5, 1, 6, 2, 6, 5, 3, 0, 1, 5, 7, 3, 2, 3, 8, 8, 8, 4, 6, 0, 4, 5, 6, 0, 1, 1, 3, 5, 7, 3, 6, 4, 6, 4, 5, 3, 1, 5, 7, 3, 8, 2, 3, 5, 4, 3, 5, 0, 1, 6, 6, 1, 1, 0, 5, 7, 7, 1, 2, 4, 1, 4, 4, 0, 5, 1, 1, 4, 8, 6, 0, 5, 6, 8, 0, 6, 2, 1, 3, 4, 7, 0, 3, 2, 1, 0, 0, 6, 4, 5, 4, 1, 1, 6, 6, 3, 5, 8, 1, 6, 6, 1, 8, 3, 5, 4, 2, 0, 1, 3, 0, 3, 1, 5, 1, 1, 5, 2, 8, 8, 1, 3, 5, 3, 2, 0, 0, 4, 0, 0, 3, 6, 8, 5, 8, 8, 6, 1, 5, 7, 6, 4, 4, 1, 3, 0, 3, 5, 8, 6, 3, 1, 1, 5, 4, 4, 0, 3, 3, 7, 5, 8, 4, 3, 3, 8, 8, 2, 7, 2, 5, 6, 0, 5, 3, 6, 2, 4, 6, 6, 4, 5, 0, 4, 2, 3, 5, 6, 5, 3, 5, 6, 4, 7, 4, 2, 1, 6, 3, 6, 5, 6, 5, 7, 4, 8, 4, 0, 2, 1, 3, 3, 0, 3, 1, 0, 4, 1, 3, 5, 3, 3, 4, 1, 5, 2, 3, 3, 2, 6, 5, 8, 6, 2, 3, 8, 1, 3, 1, 5, 5, 7, 5, 1, 3, 3, 1, 6, 8, 2, 2, 3, 1, 8, 1, 6, 6, 0, 0, 8, 7, 1, 8, 4, 0, 5, 1, 3, 0, 0, 3, 3, 5, 5, 5, 5, 3, 8, 2, 1, 3, 3, 6, 4, 4, 5, 5, 0, 8, 0, 5, 6, 7, 0, 2, 2, 2, 8, 5, 5, 1, 5, 8, 5, 7, 5, 1, 7, 0, 6, 5, 3, 6, 8, 5, 8, 5, 2, 0, 3, 5, 2, 0, 2, 5, 2, 7, 5, 8, 7, 8, 8, 2, 0, 2, 5, 2, 8, 8, 3, 1, 5, 3, 0, 3, 0, 5, 8, 6, 8, 7, 3, 5, 5, 0, 6, 4, 6, 5, 7, 5, 0, 4, 6, 8, 7, 8, 3, 5, 2, 5, 1, 1, 2, 3, 3, 0, 0, 5, 6, 5, 6, 5, 1, 6, 8, 5, 8, 4, 1, 3, 3, 0, 6, 0, 3, 4, 3, 5, 7, 1, 0, 8, 7, 6, 8, 1, 7, 0, 7, 0, 1, 1, 6, 2, 0, 5, 5, 3, 1, 3, 0, 6, 6, 0, 2, 2, 3, 4, 2, 1, 8, 5, 3, 8, 3, 6, 5, 8, 5, 0, 5, 3, 8, 3, 3, 2, 6, 4, 8, 1, 3, 5, 3, 6, 8, 4, 7, 6, 3, 4, 5, 0, 6, 5, 8, 8, 5, 5, 0, 4, 8, 8, 1, 1, 2, 0, 3, 6, 8, 8, 4, 3, 1, 2, 8, 1, 7, 2, 1, 2, 1, 3, 0, 5, 8, 7, 4, 6, 6, 1, 5, 7, 3, 3, 0, 3, 7, 8, 5, 1, 3, 6, 4, 6, 0, 1, 5, 0, 2, 6, 8, 8, 2, 7, 8, 8, 8, 1, 1, 7, 3, 6, 3, 8, 1, 4, 3, 2, 6, 5, 7, 3, 6, 0, 3, 2, 0, 5, 3, 1, 1, 1, 1, 1, 4, 0, 6, 0, 2, 0, 4, 5, 8, 5, 3, 5, 1, 1, 3, 5, 7, 5, 5, 1, 8, 3, 4, 3, 5, 0, 3, 3, 4, 0, 5, 3, 1, 6, 4, 0, 5, 1, 3, 1, 8, 6, 4, 3, 3, 0, 1, 7, 7, 3, 4, 7, 1, 5, 4, 7, 8, 2, 1, 3, 2, 3, 1, 4, 1, 6, 3, 0, 0, 6, 3, 3]
(RolloutWorker pid=18610) and here {'agent-0-beam_fired': 74, 'agent-0-cleaning': 110.0, 'agent-0-beam_hit': 9, 'agent-0-apples_consumed': 0, 'agent-1-beam_fired': 45, 'agent-1-cleaning': 74.0, 'agent-1-beam_hit': 4, 'agent-1-apples_consumed': 12, 'agent-2-beam_fired': 75, 'agent-2-cleaning': 157.0, 'agent-2-beam_hit': 3, 'agent-2-apples_consumed': 2, 'agent-3-beam_fired': 83, 'agent-3-cleaning': 83.0, 'agent-3-beam_hit': 3, 'agent-3-apples_consumed': 0, 'agent-4-beam_fired': 53, 'agent-4-cleaning': 103.0, 'agent-4-beam_hit': 3, 'agent-4-apples_consumed': 18}
(RolloutWorker pid=18610) episode 260925120 (env-idx=0) started.
(RolloutWorker pid=18610) agent id  agent-0
(RolloutWorker pid=18610) agent id  agent-1
(RolloutWorker pid=18610) agent id  agent-2
(RolloutWorker pid=18610) agent id  agent-3
(RolloutWorker pid=18610) agent id  agent-4
== Status ==
Current time: 2022-08-29 17:20:59 (running for 00:01:42.69)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |     13 |          66.4461 | 2810 |    -4440 |                      0 |                -3101 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 15150
  counters:
    num_agent_steps_sampled: 15150
    num_agent_steps_trained: 15150
    num_env_steps_sampled: 3030
    num_env_steps_trained: 3030
  custom_metrics:
    agent-0-apples_consumed:
    - 4
    - 0
    - 0
    agent-0-beam_fired:
    - 145
    - 98
    - 74
    agent-0-beam_hit:
    - 24
    - 4
    - 9
    agent-0-cleaning:
    - 103.0
    - 105.0
    - 110.0
    agent-1-apples_consumed:
    - 0
    - 9
    - 12
    agent-1-beam_fired:
    - 83
    - 63
    - 45
    agent-1-beam_hit:
    - 29
    - 5
    - 4
    agent-1-cleaning:
    - 105.0
    - 87.0
    - 74.0
    agent-2-apples_consumed:
    - 2
    - 4
    - 2
    agent-2-beam_fired:
    - 115
    - 81
    - 75
    agent-2-beam_hit:
    - 11
    - 13
    - 3
    agent-2-cleaning:
    - 121.0
    - 135.0
    - 157.0
    agent-3-apples_consumed:
    - 0
    - 15
    - 0
    agent-3-beam_fired:
    - 115
    - 90
    - 83
    agent-3-beam_hit:
    - 23
    - 11
    - 3
    agent-3-cleaning:
    - 104.0
    - 85.0
    - 83.0
    agent-4-apples_consumed:
    - 5
    - 14
    - 18
    agent-4-beam_fired:
    - 72
    - 55
    - 53
    agent-4-beam_hit:
    - 8
    - 14
    - 3
    agent-4-cleaning:
    - 96.0
    - 122.0
    - 103.0
  date: 2022-08-29_17-20-59
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -1449.0
  episode_reward_mean: -3443.0
  episode_reward_min: -5779.0
  episodes_this_iter: 1
  episodes_total: 3
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 15150
    num_agent_steps_trained: 15150
    num_env_steps_sampled: 3030
    num_env_steps_trained: 3030
  iterations_since_restore: 14
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 15150
  num_agent_steps_trained: 15150
  num_env_steps_sampled: 3030
  num_env_steps_sampled_this_iter: 220
  num_env_steps_trained: 3030
  num_env_steps_trained_this_iter: 220
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 220
  perf:
    cpu_util_percent: 10.833333333333334
    gpu_util_percent0: 0.0
    ram_util_percent: 23.833333333333332
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -400.0
    agent-1: -233.0
    agent-2: -223.0
    agent-3: -284.0
    agent-4: -185.0
  policy_reward_mean:
    agent-0: -806.0
    agent-1: -724.0
    agent-2: -588.6666666666666
    agent-3: -826.0
    agent-4: -498.3333333333333
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.18415805582659514
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.0804558684401329
    mean_inference_ms: 20.39059496741318
    mean_raw_obs_processing_ms: 2.41243476387632
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed:
      - 4
      - 0
      - 0
      agent-0-beam_fired:
      - 145
      - 98
      - 74
      agent-0-beam_hit:
      - 24
      - 4
      - 9
      agent-0-cleaning:
      - 103.0
      - 105.0
      - 110.0
      agent-1-apples_consumed:
      - 0
      - 9
      - 12
      agent-1-beam_fired:
      - 83
      - 63
      - 45
      agent-1-beam_hit:
      - 29
      - 5
      - 4
      agent-1-cleaning:
      - 105.0
      - 87.0
      - 74.0
      agent-2-apples_consumed:
      - 2
      - 4
      - 2
      agent-2-beam_fired:
      - 115
      - 81
      - 75
      agent-2-beam_hit:
      - 11
      - 13
      - 3
      agent-2-cleaning:
      - 121.0
      - 135.0
      - 157.0
      agent-3-apples_consumed:
      - 0
      - 15
      - 0
      agent-3-beam_fired:
      - 115
      - 90
      - 83
      agent-3-beam_hit:
      - 23
      - 11
      - 3
      agent-3-cleaning:
      - 104.0
      - 85.0
      - 83.0
      agent-4-apples_consumed:
      - 5
      - 14
      - 18
      agent-4-beam_fired:
      - 72
      - 55
      - 53
      agent-4-beam_hit:
      - 8
      - 14
      - 3
      agent-4-cleaning:
      - 96.0
      - 122.0
      - 103.0
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -1449.0
    episode_reward_mean: -3443.0
    episode_reward_min: -5779.0
    episodes_this_iter: 1
    hist_stats:
      episode_lengths:
      - 1000
      - 1000
      - 1000
      episode_reward:
      - -5779.0
      - -3101.0
      - -1449.0
      policy_agent-0_reward:
      - -1494.0
      - -400.0
      - -524.0
      policy_agent-1_reward:
      - -1635.0
      - -304.0
      - -233.0
      policy_agent-2_reward:
      - -714.0
      - -829.0
      - -223.0
      policy_agent-3_reward:
      - -1418.0
      - -776.0
      - -284.0
      policy_agent-4_reward:
      - -518.0
      - -792.0
      - -185.0
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -400.0
      agent-1: -233.0
      agent-2: -223.0
      agent-3: -284.0
      agent-4: -185.0
    policy_reward_mean:
      agent-0: -806.0
      agent-1: -724.0
      agent-2: -588.6666666666666
      agent-3: -826.0
      agent-4: -498.3333333333333
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.18415805582659514
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.0804558684401329
      mean_inference_ms: 20.39059496741318
      mean_raw_obs_processing_ms: 2.41243476387632
  time_since_restore: 71.61628365516663
  time_this_iter_s: 5.170194387435913
  time_total_s: 71.61628365516663
  timers:
    apply_grad_throughput: 2404.831
    apply_grad_time_ms: 20.791
    grad_wait_time_ms: 0.132
    synch_weights_time_ms: 5.542
    training_iteration_time_ms: 0.144
  timestamp: 1661790059
  timesteps_since_restore: 0
  timesteps_total: 3030
  training_iteration: 14
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:21:04 (running for 00:01:48.05)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |     14 |          71.6163 | 3030 |    -3443 |                      0 |                -1449 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 16250
  counters:
    num_agent_steps_sampled: 16250
    num_agent_steps_trained: 16250
    num_env_steps_sampled: 3250
    num_env_steps_trained: 3250
  custom_metrics:
    agent-0-apples_consumed:
    - 4
    - 0
    - 0
    agent-0-beam_fired:
    - 145
    - 98
    - 74
    agent-0-beam_hit:
    - 24
    - 4
    - 9
    agent-0-cleaning:
    - 103.0
    - 105.0
    - 110.0
    agent-1-apples_consumed:
    - 0
    - 9
    - 12
    agent-1-beam_fired:
    - 83
    - 63
    - 45
    agent-1-beam_hit:
    - 29
    - 5
    - 4
    agent-1-cleaning:
    - 105.0
    - 87.0
    - 74.0
    agent-2-apples_consumed:
    - 2
    - 4
    - 2
    agent-2-beam_fired:
    - 115
    - 81
    - 75
    agent-2-beam_hit:
    - 11
    - 13
    - 3
    agent-2-cleaning:
    - 121.0
    - 135.0
    - 157.0
    agent-3-apples_consumed:
    - 0
    - 15
    - 0
    agent-3-beam_fired:
    - 115
    - 90
    - 83
    agent-3-beam_hit:
    - 23
    - 11
    - 3
    agent-3-cleaning:
    - 104.0
    - 85.0
    - 83.0
    agent-4-apples_consumed:
    - 5
    - 14
    - 18
    agent-4-beam_fired:
    - 72
    - 55
    - 53
    agent-4-beam_hit:
    - 8
    - 14
    - 3
    agent-4-cleaning:
    - 96.0
    - 122.0
    - 103.0
  date: 2022-08-29_17-21-04
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -1449.0
  episode_reward_mean: -3443.0
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 3
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner:
      agent-0:
        learner_stats:
          allreduce_latency: 0.0
          cur_lr: 0.0001
          entropy_coeff: 0.01
          grad_gnorm: 40.0
          policy_entropy: 20.408161163330078
          policy_loss: -5.167349815368652
          vf_loss: 1.7153677940368652
      agent-1:
        learner_stats:
          allreduce_latency: 0.0
          cur_lr: 0.0001
          entropy_coeff: 0.01
          grad_gnorm: 40.0
          policy_entropy: 20.48128318786621
          policy_loss: -12.632970809936523
          vf_loss: 3.9467062950134277
      agent-2:
        learner_stats:
          allreduce_latency: 0.0
          cur_lr: 0.0001
          entropy_coeff: 0.01
          grad_gnorm: 40.0
          policy_entropy: 20.53341293334961
          policy_loss: -1.8376301527023315
          vf_loss: 0.4047609269618988
      agent-3:
        learner_stats:
          allreduce_latency: 0.0
          cur_lr: 0.0001
          entropy_coeff: 0.01
          grad_gnorm: 40.0
          policy_entropy: 19.964685440063477
          policy_loss: -27.88068199157715
          vf_loss: 8.48322582244873
      agent-4:
        learner_stats:
          allreduce_latency: 0.0
          cur_lr: 0.0001
          entropy_coeff: 0.01
          grad_gnorm: 40.0
          policy_entropy: 19.78973960876465
          policy_loss: 2.6027231216430664
          vf_loss: 1.5740282535552979
    num_agent_steps_sampled: 16250
    num_agent_steps_trained: 16250
    num_env_steps_sampled: 3250
    num_env_steps_trained: 3250
  iterations_since_restore: 15
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 16250
  num_agent_steps_trained: 16250
  num_env_steps_sampled: 3250
  num_env_steps_sampled_this_iter: 220
  num_env_steps_trained: 3250
  num_env_steps_trained_this_iter: 220
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 220
  perf:
    cpu_util_percent: 10.666666666666666
    gpu_util_percent0: 0.0
    ram_util_percent: 23.933333333333334
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -400.0
    agent-1: -233.0
    agent-2: -223.0
    agent-3: -284.0
    agent-4: -185.0
  policy_reward_mean:
    agent-0: -806.0
    agent-1: -724.0
    agent-2: -588.6666666666666
    agent-3: -826.0
    agent-4: -498.3333333333333
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.18415805582659514
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.0804558684401329
    mean_inference_ms: 20.39059496741318
    mean_raw_obs_processing_ms: 2.41243476387632
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed:
      - 4
      - 0
      - 0
      agent-0-beam_fired:
      - 145
      - 98
      - 74
      agent-0-beam_hit:
      - 24
      - 4
      - 9
      agent-0-cleaning:
      - 103.0
      - 105.0
      - 110.0
      agent-1-apples_consumed:
      - 0
      - 9
      - 12
      agent-1-beam_fired:
      - 83
      - 63
      - 45
      agent-1-beam_hit:
      - 29
      - 5
      - 4
      agent-1-cleaning:
      - 105.0
      - 87.0
      - 74.0
      agent-2-apples_consumed:
      - 2
      - 4
      - 2
      agent-2-beam_fired:
      - 115
      - 81
      - 75
      agent-2-beam_hit:
      - 11
      - 13
      - 3
      agent-2-cleaning:
      - 121.0
      - 135.0
      - 157.0
      agent-3-apples_consumed:
      - 0
      - 15
      - 0
      agent-3-beam_fired:
      - 115
      - 90
      - 83
      agent-3-beam_hit:
      - 23
      - 11
      - 3
      agent-3-cleaning:
      - 104.0
      - 85.0
      - 83.0
      agent-4-apples_consumed:
      - 5
      - 14
      - 18
      agent-4-beam_fired:
      - 72
      - 55
      - 53
      agent-4-beam_hit:
      - 8
      - 14
      - 3
      agent-4-cleaning:
      - 96.0
      - 122.0
      - 103.0
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -1449.0
    episode_reward_mean: -3443.0
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths:
      - 1000
      - 1000
      - 1000
      episode_reward:
      - -5779.0
      - -3101.0
      - -1449.0
      policy_agent-0_reward:
      - -1494.0
      - -400.0
      - -524.0
      policy_agent-1_reward:
      - -1635.0
      - -304.0
      - -233.0
      policy_agent-2_reward:
      - -714.0
      - -829.0
      - -223.0
      policy_agent-3_reward:
      - -1418.0
      - -776.0
      - -284.0
      policy_agent-4_reward:
      - -518.0
      - -792.0
      - -185.0
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -400.0
      agent-1: -233.0
      agent-2: -223.0
      agent-3: -284.0
      agent-4: -185.0
    policy_reward_mean:
      agent-0: -806.0
      agent-1: -724.0
      agent-2: -588.6666666666666
      agent-3: -826.0
      agent-4: -498.3333333333333
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.18415805582659514
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.0804558684401329
      mean_inference_ms: 20.39059496741318
      mean_raw_obs_processing_ms: 2.41243476387632
  time_since_restore: 76.65649580955505
  time_this_iter_s: 5.040212154388428
  time_total_s: 76.65649580955505
  timers:
    apply_grad_throughput: 2559.741
    apply_grad_time_ms: 19.533
    grad_wait_time_ms: 0.967
    synch_weights_time_ms: 5.947
    training_iteration_time_ms: 3.752
  timestamp: 1661790064
  timesteps_since_restore: 0
  timesteps_total: 3250
  training_iteration: 15
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:21:09 (running for 00:01:53.19)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |     15 |          76.6565 | 3250 |    -3443 |                      0 |                -1449 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 17250
  counters:
    num_agent_steps_sampled: 17250
    num_agent_steps_trained: 17250
    num_env_steps_sampled: 3450
    num_env_steps_trained: 3450
  custom_metrics:
    agent-0-apples_consumed:
    - 4
    - 0
    - 0
    agent-0-beam_fired:
    - 145
    - 98
    - 74
    agent-0-beam_hit:
    - 24
    - 4
    - 9
    agent-0-cleaning:
    - 103.0
    - 105.0
    - 110.0
    agent-1-apples_consumed:
    - 0
    - 9
    - 12
    agent-1-beam_fired:
    - 83
    - 63
    - 45
    agent-1-beam_hit:
    - 29
    - 5
    - 4
    agent-1-cleaning:
    - 105.0
    - 87.0
    - 74.0
    agent-2-apples_consumed:
    - 2
    - 4
    - 2
    agent-2-beam_fired:
    - 115
    - 81
    - 75
    agent-2-beam_hit:
    - 11
    - 13
    - 3
    agent-2-cleaning:
    - 121.0
    - 135.0
    - 157.0
    agent-3-apples_consumed:
    - 0
    - 15
    - 0
    agent-3-beam_fired:
    - 115
    - 90
    - 83
    agent-3-beam_hit:
    - 23
    - 11
    - 3
    agent-3-cleaning:
    - 104.0
    - 85.0
    - 83.0
    agent-4-apples_consumed:
    - 5
    - 14
    - 18
    agent-4-beam_fired:
    - 72
    - 55
    - 53
    agent-4-beam_hit:
    - 8
    - 14
    - 3
    agent-4-cleaning:
    - 96.0
    - 122.0
    - 103.0
  date: 2022-08-29_17-21-10
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -1449.0
  episode_reward_mean: -3443.0
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 3
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 17250
    num_agent_steps_trained: 17250
    num_env_steps_sampled: 3450
    num_env_steps_trained: 3450
  iterations_since_restore: 16
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 17250
  num_agent_steps_trained: 17250
  num_env_steps_sampled: 3450
  num_env_steps_sampled_this_iter: 200
  num_env_steps_trained: 3450
  num_env_steps_trained_this_iter: 200
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 200
  perf:
    cpu_util_percent: 11.800000000000002
    gpu_util_percent0: 0.0
    ram_util_percent: 23.96666666666667
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -400.0
    agent-1: -233.0
    agent-2: -223.0
    agent-3: -284.0
    agent-4: -185.0
  policy_reward_mean:
    agent-0: -806.0
    agent-1: -724.0
    agent-2: -588.6666666666666
    agent-3: -826.0
    agent-4: -498.3333333333333
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.18415805582659514
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.0804558684401329
    mean_inference_ms: 20.39059496741318
    mean_raw_obs_processing_ms: 2.41243476387632
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed:
      - 4
      - 0
      - 0
      agent-0-beam_fired:
      - 145
      - 98
      - 74
      agent-0-beam_hit:
      - 24
      - 4
      - 9
      agent-0-cleaning:
      - 103.0
      - 105.0
      - 110.0
      agent-1-apples_consumed:
      - 0
      - 9
      - 12
      agent-1-beam_fired:
      - 83
      - 63
      - 45
      agent-1-beam_hit:
      - 29
      - 5
      - 4
      agent-1-cleaning:
      - 105.0
      - 87.0
      - 74.0
      agent-2-apples_consumed:
      - 2
      - 4
      - 2
      agent-2-beam_fired:
      - 115
      - 81
      - 75
      agent-2-beam_hit:
      - 11
      - 13
      - 3
      agent-2-cleaning:
      - 121.0
      - 135.0
      - 157.0
      agent-3-apples_consumed:
      - 0
      - 15
      - 0
      agent-3-beam_fired:
      - 115
      - 90
      - 83
      agent-3-beam_hit:
      - 23
      - 11
      - 3
      agent-3-cleaning:
      - 104.0
      - 85.0
      - 83.0
      agent-4-apples_consumed:
      - 5
      - 14
      - 18
      agent-4-beam_fired:
      - 72
      - 55
      - 53
      agent-4-beam_hit:
      - 8
      - 14
      - 3
      agent-4-cleaning:
      - 96.0
      - 122.0
      - 103.0
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -1449.0
    episode_reward_mean: -3443.0
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths:
      - 1000
      - 1000
      - 1000
      episode_reward:
      - -5779.0
      - -3101.0
      - -1449.0
      policy_agent-0_reward:
      - -1494.0
      - -400.0
      - -524.0
      policy_agent-1_reward:
      - -1635.0
      - -304.0
      - -233.0
      policy_agent-2_reward:
      - -714.0
      - -829.0
      - -223.0
      policy_agent-3_reward:
      - -1418.0
      - -776.0
      - -284.0
      policy_agent-4_reward:
      - -518.0
      - -792.0
      - -185.0
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -400.0
      agent-1: -233.0
      agent-2: -223.0
      agent-3: -284.0
      agent-4: -185.0
    policy_reward_mean:
      agent-0: -806.0
      agent-1: -724.0
      agent-2: -588.6666666666666
      agent-3: -826.0
      agent-4: -498.3333333333333
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.18415805582659514
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.0804558684401329
      mean_inference_ms: 20.39059496741318
      mean_raw_obs_processing_ms: 2.41243476387632
  time_since_restore: 81.7630295753479
  time_this_iter_s: 5.106533765792847
  time_total_s: 81.7630295753479
  timers:
    apply_grad_throughput: 1985.962
    apply_grad_time_ms: 25.177
    grad_wait_time_ms: 0.159
    synch_weights_time_ms: 6.064
    training_iteration_time_ms: 0.172
  timestamp: 1661790070
  timesteps_since_restore: 0
  timesteps_total: 3450
  training_iteration: 16
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:21:15 (running for 00:01:58.47)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |     16 |           81.763 | 3450 |    -3443 |                      0 |                -1449 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 18300
  counters:
    num_agent_steps_sampled: 18300
    num_agent_steps_trained: 18300
    num_env_steps_sampled: 3660
    num_env_steps_trained: 3660
  custom_metrics:
    agent-0-apples_consumed:
    - 4
    - 0
    - 0
    agent-0-beam_fired:
    - 145
    - 98
    - 74
    agent-0-beam_hit:
    - 24
    - 4
    - 9
    agent-0-cleaning:
    - 103.0
    - 105.0
    - 110.0
    agent-1-apples_consumed:
    - 0
    - 9
    - 12
    agent-1-beam_fired:
    - 83
    - 63
    - 45
    agent-1-beam_hit:
    - 29
    - 5
    - 4
    agent-1-cleaning:
    - 105.0
    - 87.0
    - 74.0
    agent-2-apples_consumed:
    - 2
    - 4
    - 2
    agent-2-beam_fired:
    - 115
    - 81
    - 75
    agent-2-beam_hit:
    - 11
    - 13
    - 3
    agent-2-cleaning:
    - 121.0
    - 135.0
    - 157.0
    agent-3-apples_consumed:
    - 0
    - 15
    - 0
    agent-3-beam_fired:
    - 115
    - 90
    - 83
    agent-3-beam_hit:
    - 23
    - 11
    - 3
    agent-3-cleaning:
    - 104.0
    - 85.0
    - 83.0
    agent-4-apples_consumed:
    - 5
    - 14
    - 18
    agent-4-beam_fired:
    - 72
    - 55
    - 53
    agent-4-beam_hit:
    - 8
    - 14
    - 3
    agent-4-cleaning:
    - 96.0
    - 122.0
    - 103.0
  date: 2022-08-29_17-21-15
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -1449.0
  episode_reward_mean: -3443.0
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 3
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 18300
    num_agent_steps_trained: 18300
    num_env_steps_sampled: 3660
    num_env_steps_trained: 3660
  iterations_since_restore: 17
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 18300
  num_agent_steps_trained: 18300
  num_env_steps_sampled: 3660
  num_env_steps_sampled_this_iter: 210
  num_env_steps_trained: 3660
  num_env_steps_trained_this_iter: 210
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 210
  perf:
    cpu_util_percent: 11.1
    gpu_util_percent0: 0.0
    ram_util_percent: 23.866666666666664
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -400.0
    agent-1: -233.0
    agent-2: -223.0
    agent-3: -284.0
    agent-4: -185.0
  policy_reward_mean:
    agent-0: -806.0
    agent-1: -724.0
    agent-2: -588.6666666666666
    agent-3: -826.0
    agent-4: -498.3333333333333
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.18415805582659514
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.0804558684401329
    mean_inference_ms: 20.39059496741318
    mean_raw_obs_processing_ms: 2.41243476387632
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed:
      - 4
      - 0
      - 0
      agent-0-beam_fired:
      - 145
      - 98
      - 74
      agent-0-beam_hit:
      - 24
      - 4
      - 9
      agent-0-cleaning:
      - 103.0
      - 105.0
      - 110.0
      agent-1-apples_consumed:
      - 0
      - 9
      - 12
      agent-1-beam_fired:
      - 83
      - 63
      - 45
      agent-1-beam_hit:
      - 29
      - 5
      - 4
      agent-1-cleaning:
      - 105.0
      - 87.0
      - 74.0
      agent-2-apples_consumed:
      - 2
      - 4
      - 2
      agent-2-beam_fired:
      - 115
      - 81
      - 75
      agent-2-beam_hit:
      - 11
      - 13
      - 3
      agent-2-cleaning:
      - 121.0
      - 135.0
      - 157.0
      agent-3-apples_consumed:
      - 0
      - 15
      - 0
      agent-3-beam_fired:
      - 115
      - 90
      - 83
      agent-3-beam_hit:
      - 23
      - 11
      - 3
      agent-3-cleaning:
      - 104.0
      - 85.0
      - 83.0
      agent-4-apples_consumed:
      - 5
      - 14
      - 18
      agent-4-beam_fired:
      - 72
      - 55
      - 53
      agent-4-beam_hit:
      - 8
      - 14
      - 3
      agent-4-cleaning:
      - 96.0
      - 122.0
      - 103.0
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -1449.0
    episode_reward_mean: -3443.0
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths:
      - 1000
      - 1000
      - 1000
      episode_reward:
      - -5779.0
      - -3101.0
      - -1449.0
      policy_agent-0_reward:
      - -1494.0
      - -400.0
      - -524.0
      policy_agent-1_reward:
      - -1635.0
      - -304.0
      - -233.0
      policy_agent-2_reward:
      - -714.0
      - -829.0
      - -223.0
      policy_agent-3_reward:
      - -1418.0
      - -776.0
      - -284.0
      policy_agent-4_reward:
      - -518.0
      - -792.0
      - -185.0
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -400.0
      agent-1: -233.0
      agent-2: -223.0
      agent-3: -284.0
      agent-4: -185.0
    policy_reward_mean:
      agent-0: -806.0
      agent-1: -724.0
      agent-2: -588.6666666666666
      agent-3: -826.0
      agent-4: -498.3333333333333
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.18415805582659514
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.0804558684401329
      mean_inference_ms: 20.39059496741318
      mean_raw_obs_processing_ms: 2.41243476387632
  time_since_restore: 86.938809633255
  time_this_iter_s: 5.1757800579071045
  time_total_s: 86.938809633255
  timers:
    apply_grad_throughput: 2258.409
    apply_grad_time_ms: 22.139
    grad_wait_time_ms: 0.152
    synch_weights_time_ms: 5.397
    training_iteration_time_ms: 0.165
  timestamp: 1661790075
  timesteps_since_restore: 0
  timesteps_total: 3660
  training_iteration: 17
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:21:20 (running for 00:02:03.75)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |     17 |          86.9388 | 3660 |    -3443 |                      0 |                -1449 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 19300
  counters:
    num_agent_steps_sampled: 19300
    num_agent_steps_trained: 19300
    num_env_steps_sampled: 3860
    num_env_steps_trained: 3860
  custom_metrics:
    agent-0-apples_consumed:
    - 4
    - 0
    - 0
    agent-0-beam_fired:
    - 145
    - 98
    - 74
    agent-0-beam_hit:
    - 24
    - 4
    - 9
    agent-0-cleaning:
    - 103.0
    - 105.0
    - 110.0
    agent-1-apples_consumed:
    - 0
    - 9
    - 12
    agent-1-beam_fired:
    - 83
    - 63
    - 45
    agent-1-beam_hit:
    - 29
    - 5
    - 4
    agent-1-cleaning:
    - 105.0
    - 87.0
    - 74.0
    agent-2-apples_consumed:
    - 2
    - 4
    - 2
    agent-2-beam_fired:
    - 115
    - 81
    - 75
    agent-2-beam_hit:
    - 11
    - 13
    - 3
    agent-2-cleaning:
    - 121.0
    - 135.0
    - 157.0
    agent-3-apples_consumed:
    - 0
    - 15
    - 0
    agent-3-beam_fired:
    - 115
    - 90
    - 83
    agent-3-beam_hit:
    - 23
    - 11
    - 3
    agent-3-cleaning:
    - 104.0
    - 85.0
    - 83.0
    agent-4-apples_consumed:
    - 5
    - 14
    - 18
    agent-4-beam_fired:
    - 72
    - 55
    - 53
    agent-4-beam_hit:
    - 8
    - 14
    - 3
    agent-4-cleaning:
    - 96.0
    - 122.0
    - 103.0
  date: 2022-08-29_17-21-20
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -1449.0
  episode_reward_mean: -3443.0
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 3
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 19300
    num_agent_steps_trained: 19300
    num_env_steps_sampled: 3860
    num_env_steps_trained: 3860
  iterations_since_restore: 18
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 19300
  num_agent_steps_trained: 19300
  num_env_steps_sampled: 3860
  num_env_steps_sampled_this_iter: 200
  num_env_steps_trained: 3860
  num_env_steps_trained_this_iter: 200
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 200
  perf:
    cpu_util_percent: 11.225
    gpu_util_percent0: 0.0
    ram_util_percent: 23.875
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -400.0
    agent-1: -233.0
    agent-2: -223.0
    agent-3: -284.0
    agent-4: -185.0
  policy_reward_mean:
    agent-0: -806.0
    agent-1: -724.0
    agent-2: -588.6666666666666
    agent-3: -826.0
    agent-4: -498.3333333333333
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.18415805582659514
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.0804558684401329
    mean_inference_ms: 20.39059496741318
    mean_raw_obs_processing_ms: 2.41243476387632
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed:
      - 4
      - 0
      - 0
      agent-0-beam_fired:
      - 145
      - 98
      - 74
      agent-0-beam_hit:
      - 24
      - 4
      - 9
      agent-0-cleaning:
      - 103.0
      - 105.0
      - 110.0
      agent-1-apples_consumed:
      - 0
      - 9
      - 12
      agent-1-beam_fired:
      - 83
      - 63
      - 45
      agent-1-beam_hit:
      - 29
      - 5
      - 4
      agent-1-cleaning:
      - 105.0
      - 87.0
      - 74.0
      agent-2-apples_consumed:
      - 2
      - 4
      - 2
      agent-2-beam_fired:
      - 115
      - 81
      - 75
      agent-2-beam_hit:
      - 11
      - 13
      - 3
      agent-2-cleaning:
      - 121.0
      - 135.0
      - 157.0
      agent-3-apples_consumed:
      - 0
      - 15
      - 0
      agent-3-beam_fired:
      - 115
      - 90
      - 83
      agent-3-beam_hit:
      - 23
      - 11
      - 3
      agent-3-cleaning:
      - 104.0
      - 85.0
      - 83.0
      agent-4-apples_consumed:
      - 5
      - 14
      - 18
      agent-4-beam_fired:
      - 72
      - 55
      - 53
      agent-4-beam_hit:
      - 8
      - 14
      - 3
      agent-4-cleaning:
      - 96.0
      - 122.0
      - 103.0
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -1449.0
    episode_reward_mean: -3443.0
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths:
      - 1000
      - 1000
      - 1000
      episode_reward:
      - -5779.0
      - -3101.0
      - -1449.0
      policy_agent-0_reward:
      - -1494.0
      - -400.0
      - -524.0
      policy_agent-1_reward:
      - -1635.0
      - -304.0
      - -233.0
      policy_agent-2_reward:
      - -714.0
      - -829.0
      - -223.0
      policy_agent-3_reward:
      - -1418.0
      - -776.0
      - -284.0
      policy_agent-4_reward:
      - -518.0
      - -792.0
      - -185.0
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -400.0
      agent-1: -233.0
      agent-2: -223.0
      agent-3: -284.0
      agent-4: -185.0
    policy_reward_mean:
      agent-0: -806.0
      agent-1: -724.0
      agent-2: -588.6666666666666
      agent-3: -826.0
      agent-4: -498.3333333333333
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.18415805582659514
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.0804558684401329
      mean_inference_ms: 20.39059496741318
      mean_raw_obs_processing_ms: 2.41243476387632
  time_since_restore: 92.1379725933075
  time_this_iter_s: 5.19916296005249
  time_total_s: 92.1379725933075
  timers:
    apply_grad_throughput: 2221.141
    apply_grad_time_ms: 22.511
    grad_wait_time_ms: 0.174
    synch_weights_time_ms: 5.31
    training_iteration_time_ms: 0.191
  timestamp: 1661790080
  timesteps_since_restore: 0
  timesteps_total: 3860
  training_iteration: 18
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

(RolloutWorker pid=18610) check agent ids ['agent-0', 'agent-1', 'agent-2', 'agent-3', 'agent-4']
(RolloutWorker pid=18610) checking again ['agent-0', 'agent-1', 'agent-2', 'agent-3', 'agent-4']
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) here we are agent-4 [6, 1, 0, 1, 1, 5, 2, 6, 3, 1, 5, 5, 3, 6, 3, 0, 4, 8, 6, 8, 4, 4, 4, 4, 0, 3, 1, 2, 0, 2, 8, 0, 6, 2, 3, 5, 1, 2, 6, 3, 4, 1, 8, 4, 1, 8, 3, 2, 8, 3, 1, 3, 5, 1, 3, 6, 5, 1, 2, 4, 1, 3, 0, 7, 6, 5, 8, 0, 0, 0, 0, 0, 0, 3, 0, 6, 4, 3, 3, 3, 8, 8, 5, 1, 0, 5, 4, 4, 8, 7, 0, 4, 4, 0, 5, 6, 2, 5, 8, 8, 6, 6, 6, 3, 2, 3, 4, 1, 6, 6, 6, 8, 8, 1, 0, 8, 8, 5, 8, 0, 6, 1, 3, 6, 6, 5, 5, 0, 0, 5, 0, 6, 1, 8, 3, 3, 8, 3, 1, 0, 3, 0, 5, 4, 5, 2, 0, 6, 8, 3, 7, 5, 7, 3, 1, 3, 8, 2, 6, 1, 8, 8, 4, 8, 2, 4, 3, 7, 2, 4, 1, 6, 4, 5, 8, 3, 5, 4, 7, 4, 8, 1, 1, 3, 8, 8, 3, 2, 8, 1, 1, 5, 0, 3, 6, 1, 8, 1, 3, 3, 3, 3, 2, 3, 6, 5, 5, 6, 8, 5, 8, 4, 4, 8, 8, 4, 6, 6, 0, 7, 8, 5, 6, 1, 2, 4, 0, 6, 6, 2, 4, 8, 4, 8, 6, 8, 8, 0, 6, 8, 3, 5, 5, 5, 3, 0, 2, 3, 6, 4, 7, 1, 6, 8, 8, 6, 5, 6, 5, 1, 3, 1, 0, 3, 6, 3, 5, 0, 5, 3, 2, 7, 3, 7, 3, 8, 6, 0, 6, 5, 1, 0, 7, 5, 3, 8, 3, 5, 6, 0, 8, 3, 4, 8, 3, 8, 1, 8, 2, 4, 8, 4, 3, 8, 8, 7, 8, 8, 3, 8, 5, 1, 7, 2, 4, 6, 0, 4, 2, 5, 3, 7, 2, 3, 1, 0, 4, 5, 8, 1, 0, 3, 0, 1, 3, 7, 8, 4, 8, 6, 8, 6, 5, 3, 7, 8, 6, 1, 1, 1, 1, 6, 2, 8, 3, 8, 5, 3, 3, 8, 8, 6, 3, 5, 8, 0, 4, 2, 8, 2, 7, 3, 1, 3, 7, 6, 0, 5, 7, 4, 1, 3, 8, 4, 7, 3, 3, 4, 3, 3, 2, 3, 2, 5, 4, 8, 1, 7, 8, 3, 8, 4, 8, 0, 6, 7, 6, 6, 0, 1, 6, 5, 7, 3, 0, 3, 4, 7, 0, 3, 1, 0, 2, 1, 2, 2, 1, 6, 2, 3, 4, 0, 0, 8, 7, 4, 3, 6, 5, 6, 1, 6, 3, 1, 0, 5, 0, 0, 0, 5, 2, 5, 4, 5, 4, 5, 4, 6, 8, 1, 3, 8, 0, 1, 0, 1, 0, 2, 4, 1, 3, 5, 5, 1, 1, 1, 3, 5, 1, 3, 0, 2, 1, 1, 2, 5, 1, 6, 8, 3, 6, 3, 6, 0, 5, 1, 5, 4, 3, 7, 8, 6, 6, 0, 3, 3, 6, 6, 6, 6, 1, 8, 5, 5, 6, 0, 5, 1, 1, 5, 5, 6, 8, 5, 2, 3, 4, 2, 3, 7, 3, 6, 5, 2, 0, 0, 2, 8, 3, 4, 1, 2, 5, 1, 1, 6, 1, 3, 1, 1, 1, 3, 3, 8, 3, 3, 8, 2, 5, 5, 4, 5, 2, 1, 5, 1, 4, 5, 5, 5, 4, 5, 5, 6, 0, 6, 8, 3, 8, 4, 1, 6, 3, 0, 3, 0, 2, 1, 8, 5, 8, 8, 5, 8, 5, 0, 5, 3, 2, 0, 3, 3, 6, 1, 1, 1, 5, 6, 8, 3, 2, 3, 8, 8, 5, 3, 0, 3, 5, 6, 5, 8, 0, 1, 3, 8, 5, 8, 4, 4, 8, 2, 4, 4, 7, 3, 7, 4, 6, 1, 3, 1, 8, 4, 7, 8, 1, 5, 3, 4, 1, 3, 6, 6, 3, 8, 1, 5, 8, 5, 0, 4, 0, 5, 6, 8, 8, 5, 0, 8, 2, 6, 4, 5, 5, 4, 0, 5, 1, 1, 1, 0, 8, 4, 0, 1, 1, 8, 3, 3, 3, 1, 0, 1, 0, 1, 2, 2, 1, 8, 1, 3, 7, 6, 1, 6, 2, 6, 4, 8, 5, 7, 5, 5, 6, 8, 3, 1, 5, 4, 0, 6, 8, 4, 8, 8, 1, 7, 7, 5, 1, 1, 6, 3, 1, 6, 1, 0, 1, 0, 4, 3, 4, 4, 0, 1, 6, 3, 8, 4, 2, 8, 3, 2, 1, 3, 4, 8, 5, 4, 3, 8, 6, 7, 4, 4, 4, 4, 5, 8, 5, 8, 1, 4, 1, 1, 4, 5, 7, 2, 3, 0, 0, 0, 8, 3, 5, 8, 1, 5, 8, 1, 5, 3, 2, 4, 3, 1, 6, 0, 1, 1, 1, 6, 3, 8, 0, 0, 4, 5, 4, 1, 3, 7, 6, 3, 3, 7, 1, 2, 2, 5, 0, 5, 6, 3, 6, 0, 1, 2, 2, 5, 0, 1, 2, 2, 8, 2, 8, 5, 7, 2, 5, 0, 1, 0, 2, 2, 3, 3, 0, 8, 1, 5, 2, 6, 1, 1, 5, 6, 1, 1, 4, 5, 4, 3, 1, 5, 3, 2, 7, 7, 4, 3, 3, 1, 1, 1, 5, 3, 3, 4, 3, 3, 2, 7, 8, 4, 3, 0, 0, 8, 3, 3, 0, 6, 1, 5, 8, 1, 5, 1, 3, 5, 8, 2, 1, 8, 8, 1, 6, 3, 0, 7, 3, 1, 5, 5, 1, 2, 3, 6, 4, 2, 8, 3, 1, 1, 5, 5, 3, 3, 0, 2, 3, 3, 5, 5, 3, 8, 7, 5, 0, 1, 4, 1, 3, 2, 4, 2, 3, 3, 5, 4, 4, 8, 3, 5, 1, 1, 0, 0, 8, 0, 3, 8, 5, 0, 3, 3, 5, 8, 8, 8, 8, 8, 0, 4, 4, 6, 6, 1, 6, 6, 8, 8, 8, 8, 8, 6, 3, 2, 1, 1, 8, 8, 1, 8, 8, 3]
(RolloutWorker pid=18610) and here {'agent-0-beam_fired': 63, 'agent-0-cleaning': 107.0, 'agent-0-beam_hit': 7, 'agent-0-apples_consumed': 7, 'agent-1-beam_fired': 43, 'agent-1-cleaning': 57.0, 'agent-1-beam_hit': 9, 'agent-1-apples_consumed': 5, 'agent-2-beam_fired': 50, 'agent-2-cleaning': 128.0, 'agent-2-beam_hit': 4, 'agent-2-apples_consumed': 7, 'agent-3-beam_fired': 55, 'agent-3-cleaning': 109.0, 'agent-3-beam_hit': 3, 'agent-3-apples_consumed': 1, 'agent-4-beam_fired': 44, 'agent-4-cleaning': 146.0, 'agent-4-beam_hit': 4, 'agent-4-apples_consumed': 16}
(RolloutWorker pid=18610) episode 193723046 (env-idx=0) started.
(RolloutWorker pid=18610) agent id  agent-0
(RolloutWorker pid=18610) agent id  agent-1
(RolloutWorker pid=18610) agent id  agent-2
(RolloutWorker pid=18610) agent id  agent-3
(RolloutWorker pid=18610) agent id  agent-4
== Status ==
Current time: 2022-08-29 17:21:26 (running for 00:02:09.89)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |     18 |           92.138 | 3860 |    -3443 |                      0 |                -1449 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 20450
  counters:
    num_agent_steps_sampled: 20450
    num_agent_steps_trained: 20450
    num_env_steps_sampled: 4090
    num_env_steps_trained: 4090
  custom_metrics:
    agent-0-apples_consumed: [4, 0, 0, 7]
    agent-0-beam_fired: [145, 98, 74, 63]
    agent-0-beam_hit: [24, 4, 9, 7]
    agent-0-cleaning: [103.0, 105.0, 110.0, 107.0]
    agent-1-apples_consumed: [0, 9, 12, 5]
    agent-1-beam_fired: [83, 63, 45, 43]
    agent-1-beam_hit: [29, 5, 4, 9]
    agent-1-cleaning: [105.0, 87.0, 74.0, 57.0]
    agent-2-apples_consumed: [2, 4, 2, 7]
    agent-2-beam_fired: [115, 81, 75, 50]
    agent-2-beam_hit: [11, 13, 3, 4]
    agent-2-cleaning: [121.0, 135.0, 157.0, 128.0]
    agent-3-apples_consumed: [0, 15, 0, 1]
    agent-3-beam_fired: [115, 90, 83, 55]
    agent-3-beam_hit: [23, 11, 3, 3]
    agent-3-cleaning: [104.0, 85.0, 83.0, 109.0]
    agent-4-apples_consumed: [5, 14, 18, 16]
    agent-4-beam_fired: [72, 55, 53, 44]
    agent-4-beam_hit: [8, 14, 3, 4]
    agent-4-cleaning: [96.0, 122.0, 103.0, 146.0]
  date: 2022-08-29_17-21-26
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -1449.0
  episode_reward_mean: -3000.0
  episode_reward_min: -5779.0
  episodes_this_iter: 1
  episodes_total: 4
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 20450
    num_agent_steps_trained: 20450
    num_env_steps_sampled: 4090
    num_env_steps_trained: 4090
  iterations_since_restore: 19
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 20450
  num_agent_steps_trained: 20450
  num_env_steps_sampled: 4090
  num_env_steps_sampled_this_iter: 230
  num_env_steps_trained: 4090
  num_env_steps_trained_this_iter: 230
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 230
  perf:
    cpu_util_percent: 10.166666666666668
    gpu_util_percent0: 0.0
    ram_util_percent: 24.0
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -400.0
    agent-1: -233.0
    agent-2: -223.0
    agent-3: -204.0
    agent-4: -185.0
  policy_reward_mean:
    agent-0: -718.75
    agent-1: -677.75
    agent-2: -502.25
    agent-3: -670.5
    agent-4: -430.75
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.18426561130589345
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.078740745128364
    mean_inference_ms: 20.465832203546437
    mean_raw_obs_processing_ms: 2.4191114857079503
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed: [4, 0, 0, 7]
      agent-0-beam_fired: [145, 98, 74, 63]
      agent-0-beam_hit: [24, 4, 9, 7]
      agent-0-cleaning: [103.0, 105.0, 110.0, 107.0]
      agent-1-apples_consumed: [0, 9, 12, 5]
      agent-1-beam_fired: [83, 63, 45, 43]
      agent-1-beam_hit: [29, 5, 4, 9]
      agent-1-cleaning: [105.0, 87.0, 74.0, 57.0]
      agent-2-apples_consumed: [2, 4, 2, 7]
      agent-2-beam_fired: [115, 81, 75, 50]
      agent-2-beam_hit: [11, 13, 3, 4]
      agent-2-cleaning: [121.0, 135.0, 157.0, 128.0]
      agent-3-apples_consumed: [0, 15, 0, 1]
      agent-3-beam_fired: [115, 90, 83, 55]
      agent-3-beam_hit: [23, 11, 3, 3]
      agent-3-cleaning: [104.0, 85.0, 83.0, 109.0]
      agent-4-apples_consumed: [5, 14, 18, 16]
      agent-4-beam_fired: [72, 55, 53, 44]
      agent-4-beam_hit: [8, 14, 3, 4]
      agent-4-cleaning: [96.0, 122.0, 103.0, 146.0]
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -1449.0
    episode_reward_mean: -3000.0
    episode_reward_min: -5779.0
    episodes_this_iter: 1
    hist_stats:
      episode_lengths: [1000, 1000, 1000, 1000]
      episode_reward: [-5779.0, -3101.0, -1449.0, -1671.0]
      policy_agent-0_reward: [-1494.0, -400.0, -524.0, -457.0]
      policy_agent-1_reward: [-1635.0, -304.0, -233.0, -539.0]
      policy_agent-2_reward: [-714.0, -829.0, -223.0, -243.0]
      policy_agent-3_reward: [-1418.0, -776.0, -284.0, -204.0]
      policy_agent-4_reward: [-518.0, -792.0, -185.0, -228.0]
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -400.0
      agent-1: -233.0
      agent-2: -223.0
      agent-3: -204.0
      agent-4: -185.0
    policy_reward_mean:
      agent-0: -718.75
      agent-1: -677.75
      agent-2: -502.25
      agent-3: -670.5
      agent-4: -430.75
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.18426561130589345
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.078740745128364
      mean_inference_ms: 20.465832203546437
      mean_raw_obs_processing_ms: 2.4191114857079503
  time_since_restore: 97.32737517356873
  time_this_iter_s: 5.1894025802612305
  time_total_s: 97.32737517356873
  timers:
    apply_grad_throughput: 2464.04
    apply_grad_time_ms: 20.292
    grad_wait_time_ms: 0.172
    synch_weights_time_ms: 5.619
    training_iteration_time_ms: 0.188
  timestamp: 1661790086
  timesteps_since_restore: 0
  timesteps_total: 4090
  training_iteration: 19
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:21:31 (running for 00:02:15.17)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |     19 |          97.3274 | 4090 |    -3000 |                      0 |                -1449 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 21500
  counters:
    num_agent_steps_sampled: 21500
    num_agent_steps_trained: 21500
    num_env_steps_sampled: 4300
    num_env_steps_trained: 4300
  custom_metrics:
    agent-0-apples_consumed: [4, 0, 0, 7]
    agent-0-beam_fired: [145, 98, 74, 63]
    agent-0-beam_hit: [24, 4, 9, 7]
    agent-0-cleaning: [103.0, 105.0, 110.0, 107.0]
    agent-1-apples_consumed: [0, 9, 12, 5]
    agent-1-beam_fired: [83, 63, 45, 43]
    agent-1-beam_hit: [29, 5, 4, 9]
    agent-1-cleaning: [105.0, 87.0, 74.0, 57.0]
    agent-2-apples_consumed: [2, 4, 2, 7]
    agent-2-beam_fired: [115, 81, 75, 50]
    agent-2-beam_hit: [11, 13, 3, 4]
    agent-2-cleaning: [121.0, 135.0, 157.0, 128.0]
    agent-3-apples_consumed: [0, 15, 0, 1]
    agent-3-beam_fired: [115, 90, 83, 55]
    agent-3-beam_hit: [23, 11, 3, 3]
    agent-3-cleaning: [104.0, 85.0, 83.0, 109.0]
    agent-4-apples_consumed: [5, 14, 18, 16]
    agent-4-beam_fired: [72, 55, 53, 44]
    agent-4-beam_hit: [8, 14, 3, 4]
    agent-4-cleaning: [96.0, 122.0, 103.0, 146.0]
  date: 2022-08-29_17-21-32
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -1449.0
  episode_reward_mean: -3000.0
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 4
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 21500
    num_agent_steps_trained: 21500
    num_env_steps_sampled: 4300
    num_env_steps_trained: 4300
  iterations_since_restore: 20
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 21500
  num_agent_steps_trained: 21500
  num_env_steps_sampled: 4300
  num_env_steps_sampled_this_iter: 210
  num_env_steps_trained: 4300
  num_env_steps_trained_this_iter: 210
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 210
  perf:
    cpu_util_percent: 10.866666666666665
    gpu_util_percent0: 0.0
    ram_util_percent: 24.0
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -400.0
    agent-1: -233.0
    agent-2: -223.0
    agent-3: -204.0
    agent-4: -185.0
  policy_reward_mean:
    agent-0: -718.75
    agent-1: -677.75
    agent-2: -502.25
    agent-3: -670.5
    agent-4: -430.75
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.18426561130589345
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.078740745128364
    mean_inference_ms: 20.465832203546437
    mean_raw_obs_processing_ms: 2.4191114857079503
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed: [4, 0, 0, 7]
      agent-0-beam_fired: [145, 98, 74, 63]
      agent-0-beam_hit: [24, 4, 9, 7]
      agent-0-cleaning: [103.0, 105.0, 110.0, 107.0]
      agent-1-apples_consumed: [0, 9, 12, 5]
      agent-1-beam_fired: [83, 63, 45, 43]
      agent-1-beam_hit: [29, 5, 4, 9]
      agent-1-cleaning: [105.0, 87.0, 74.0, 57.0]
      agent-2-apples_consumed: [2, 4, 2, 7]
      agent-2-beam_fired: [115, 81, 75, 50]
      agent-2-beam_hit: [11, 13, 3, 4]
      agent-2-cleaning: [121.0, 135.0, 157.0, 128.0]
      agent-3-apples_consumed: [0, 15, 0, 1]
      agent-3-beam_fired: [115, 90, 83, 55]
      agent-3-beam_hit: [23, 11, 3, 3]
      agent-3-cleaning: [104.0, 85.0, 83.0, 109.0]
      agent-4-apples_consumed: [5, 14, 18, 16]
      agent-4-beam_fired: [72, 55, 53, 44]
      agent-4-beam_hit: [8, 14, 3, 4]
      agent-4-cleaning: [96.0, 122.0, 103.0, 146.0]
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -1449.0
    episode_reward_mean: -3000.0
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths: [1000, 1000, 1000, 1000]
      episode_reward: [-5779.0, -3101.0, -1449.0, -1671.0]
      policy_agent-0_reward: [-1494.0, -400.0, -524.0, -457.0]
      policy_agent-1_reward: [-1635.0, -304.0, -233.0, -539.0]
      policy_agent-2_reward: [-714.0, -829.0, -223.0, -243.0]
      policy_agent-3_reward: [-1418.0, -776.0, -284.0, -204.0]
      policy_agent-4_reward: [-518.0, -792.0, -185.0, -228.0]
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -400.0
      agent-1: -233.0
      agent-2: -223.0
      agent-3: -204.0
      agent-4: -185.0
    policy_reward_mean:
      agent-0: -718.75
      agent-1: -677.75
      agent-2: -502.25
      agent-3: -670.5
      agent-4: -430.75
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.18426561130589345
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.078740745128364
      mean_inference_ms: 20.465832203546437
      mean_raw_obs_processing_ms: 2.4191114857079503
  time_since_restore: 102.41794538497925
  time_this_iter_s: 5.0905702114105225
  time_total_s: 102.41794538497925
  timers:
    apply_grad_throughput: 2448.954
    apply_grad_time_ms: 20.417
    grad_wait_time_ms: 0.146
    synch_weights_time_ms: 5.742
    training_iteration_time_ms: 0.157
  timestamp: 1661790092
  timesteps_since_restore: 0
  timesteps_total: 4300
  training_iteration: 20
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:21:37 (running for 00:02:20.46)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |     20 |          102.418 | 4300 |    -3000 |                      0 |                -1449 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 22550
  counters:
    num_agent_steps_sampled: 22550
    num_agent_steps_trained: 22550
    num_env_steps_sampled: 4510
    num_env_steps_trained: 4510
  custom_metrics:
    agent-0-apples_consumed: [4, 0, 0, 7]
    agent-0-beam_fired: [145, 98, 74, 63]
    agent-0-beam_hit: [24, 4, 9, 7]
    agent-0-cleaning: [103.0, 105.0, 110.0, 107.0]
    agent-1-apples_consumed: [0, 9, 12, 5]
    agent-1-beam_fired: [83, 63, 45, 43]
    agent-1-beam_hit: [29, 5, 4, 9]
    agent-1-cleaning: [105.0, 87.0, 74.0, 57.0]
    agent-2-apples_consumed: [2, 4, 2, 7]
    agent-2-beam_fired: [115, 81, 75, 50]
    agent-2-beam_hit: [11, 13, 3, 4]
    agent-2-cleaning: [121.0, 135.0, 157.0, 128.0]
    agent-3-apples_consumed: [0, 15, 0, 1]
    agent-3-beam_fired: [115, 90, 83, 55]
    agent-3-beam_hit: [23, 11, 3, 3]
    agent-3-cleaning: [104.0, 85.0, 83.0, 109.0]
    agent-4-apples_consumed: [5, 14, 18, 16]
    agent-4-beam_fired: [72, 55, 53, 44]
    agent-4-beam_hit: [8, 14, 3, 4]
    agent-4-cleaning: [96.0, 122.0, 103.0, 146.0]
  date: 2022-08-29_17-21-37
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -1449.0
  episode_reward_mean: -3000.0
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 4
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 22550
    num_agent_steps_trained: 22550
    num_env_steps_sampled: 4510
    num_env_steps_trained: 4510
  iterations_since_restore: 21
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 22550
  num_agent_steps_trained: 22550
  num_env_steps_sampled: 4510
  num_env_steps_sampled_this_iter: 210
  num_env_steps_trained: 4510
  num_env_steps_trained_this_iter: 210
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 210
  perf:
    cpu_util_percent: 10.733333333333334
    gpu_util_percent0: 0.0
    ram_util_percent: 24.03333333333333
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -400.0
    agent-1: -233.0
    agent-2: -223.0
    agent-3: -204.0
    agent-4: -185.0
  policy_reward_mean:
    agent-0: -718.75
    agent-1: -677.75
    agent-2: -502.25
    agent-3: -670.5
    agent-4: -430.75
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.18426561130589345
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.078740745128364
    mean_inference_ms: 20.465832203546437
    mean_raw_obs_processing_ms: 2.4191114857079503
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed: [4, 0, 0, 7]
      agent-0-beam_fired: [145, 98, 74, 63]
      agent-0-beam_hit: [24, 4, 9, 7]
      agent-0-cleaning: [103.0, 105.0, 110.0, 107.0]
      agent-1-apples_consumed: [0, 9, 12, 5]
      agent-1-beam_fired: [83, 63, 45, 43]
      agent-1-beam_hit: [29, 5, 4, 9]
      agent-1-cleaning: [105.0, 87.0, 74.0, 57.0]
      agent-2-apples_consumed: [2, 4, 2, 7]
      agent-2-beam_fired: [115, 81, 75, 50]
      agent-2-beam_hit: [11, 13, 3, 4]
      agent-2-cleaning: [121.0, 135.0, 157.0, 128.0]
      agent-3-apples_consumed: [0, 15, 0, 1]
      agent-3-beam_fired: [115, 90, 83, 55]
      agent-3-beam_hit: [23, 11, 3, 3]
      agent-3-cleaning: [104.0, 85.0, 83.0, 109.0]
      agent-4-apples_consumed: [5, 14, 18, 16]
      agent-4-beam_fired: [72, 55, 53, 44]
      agent-4-beam_hit: [8, 14, 3, 4]
      agent-4-cleaning: [96.0, 122.0, 103.0, 146.0]
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -1449.0
    episode_reward_mean: -3000.0
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths: [1000, 1000, 1000, 1000]
      episode_reward: [-5779.0, -3101.0, -1449.0, -1671.0]
      policy_agent-0_reward: [-1494.0, -400.0, -524.0, -457.0]
      policy_agent-1_reward: [-1635.0, -304.0, -233.0, -539.0]
      policy_agent-2_reward: [-714.0, -829.0, -223.0, -243.0]
      policy_agent-3_reward: [-1418.0, -776.0, -284.0, -204.0]
      policy_agent-4_reward: [-518.0, -792.0, -185.0, -228.0]
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -400.0
      agent-1: -233.0
      agent-2: -223.0
      agent-3: -204.0
      agent-4: -185.0
    policy_reward_mean:
      agent-0: -718.75
      agent-1: -677.75
      agent-2: -502.25
      agent-3: -670.5
      agent-4: -430.75
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.18426561130589345
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.078740745128364
      mean_inference_ms: 20.465832203546437
      mean_raw_obs_processing_ms: 2.4191114857079503
  time_since_restore: 107.45789098739624
  time_this_iter_s: 5.039945602416992
  time_total_s: 107.45789098739624
  timers:
    apply_grad_throughput: 2326.56
    apply_grad_time_ms: 21.491
    grad_wait_time_ms: 0.184
    synch_weights_time_ms: 5.941
    training_iteration_time_ms: 0.198
  timestamp: 1661790097
  timesteps_since_restore: 0
  timesteps_total: 4510
  training_iteration: 21
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:21:42 (running for 00:02:25.58)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |     21 |          107.458 | 4510 |    -3000 |                      0 |                -1449 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 23600
  counters:
    num_agent_steps_sampled: 23600
    num_agent_steps_trained: 23600
    num_env_steps_sampled: 4720
    num_env_steps_trained: 4720
  custom_metrics:
    agent-0-apples_consumed: [4, 0, 0, 7]
    agent-0-beam_fired: [145, 98, 74, 63]
    agent-0-beam_hit: [24, 4, 9, 7]
    agent-0-cleaning: [103.0, 105.0, 110.0, 107.0]
    agent-1-apples_consumed: [0, 9, 12, 5]
    agent-1-beam_fired: [83, 63, 45, 43]
    agent-1-beam_hit: [29, 5, 4, 9]
    agent-1-cleaning: [105.0, 87.0, 74.0, 57.0]
    agent-2-apples_consumed: [2, 4, 2, 7]
    agent-2-beam_fired: [115, 81, 75, 50]
    agent-2-beam_hit: [11, 13, 3, 4]
    agent-2-cleaning: [121.0, 135.0, 157.0, 128.0]
    agent-3-apples_consumed: [0, 15, 0, 1]
    agent-3-beam_fired: [115, 90, 83, 55]
    agent-3-beam_hit: [23, 11, 3, 3]
    agent-3-cleaning: [104.0, 85.0, 83.0, 109.0]
    agent-4-apples_consumed: [5, 14, 18, 16]
    agent-4-beam_fired: [72, 55, 53, 44]
    agent-4-beam_hit: [8, 14, 3, 4]
    agent-4-cleaning: [96.0, 122.0, 103.0, 146.0]
  date: 2022-08-29_17-21-42
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -1449.0
  episode_reward_mean: -3000.0
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 4
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 23600
    num_agent_steps_trained: 23600
    num_env_steps_sampled: 4720
    num_env_steps_trained: 4720
  iterations_since_restore: 22
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 23600
  num_agent_steps_trained: 23600
  num_env_steps_sampled: 4720
  num_env_steps_sampled_this_iter: 210
  num_env_steps_trained: 4720
  num_env_steps_trained_this_iter: 210
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 210
  perf:
    cpu_util_percent: 10.800000000000002
    gpu_util_percent0: 0.0
    ram_util_percent: 23.96666666666667
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -400.0
    agent-1: -233.0
    agent-2: -223.0
    agent-3: -204.0
    agent-4: -185.0
  policy_reward_mean:
    agent-0: -718.75
    agent-1: -677.75
    agent-2: -502.25
    agent-3: -670.5
    agent-4: -430.75
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.18426561130589345
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.078740745128364
    mean_inference_ms: 20.465832203546437
    mean_raw_obs_processing_ms: 2.4191114857079503
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed: [4, 0, 0, 7]
      agent-0-beam_fired: [145, 98, 74, 63]
      agent-0-beam_hit: [24, 4, 9, 7]
      agent-0-cleaning: [103.0, 105.0, 110.0, 107.0]
      agent-1-apples_consumed: [0, 9, 12, 5]
      agent-1-beam_fired: [83, 63, 45, 43]
      agent-1-beam_hit: [29, 5, 4, 9]
      agent-1-cleaning: [105.0, 87.0, 74.0, 57.0]
      agent-2-apples_consumed: [2, 4, 2, 7]
      agent-2-beam_fired: [115, 81, 75, 50]
      agent-2-beam_hit: [11, 13, 3, 4]
      agent-2-cleaning: [121.0, 135.0, 157.0, 128.0]
      agent-3-apples_consumed: [0, 15, 0, 1]
      agent-3-beam_fired: [115, 90, 83, 55]
      agent-3-beam_hit: [23, 11, 3, 3]
      agent-3-cleaning: [104.0, 85.0, 83.0, 109.0]
      agent-4-apples_consumed: [5, 14, 18, 16]
      agent-4-beam_fired: [72, 55, 53, 44]
      agent-4-beam_hit: [8, 14, 3, 4]
      agent-4-cleaning: [96.0, 122.0, 103.0, 146.0]
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -1449.0
    episode_reward_mean: -3000.0
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths: [1000, 1000, 1000, 1000]
      episode_reward: [-5779.0, -3101.0, -1449.0, -1671.0]
      policy_agent-0_reward: [-1494.0, -400.0, -524.0, -457.0]
      policy_agent-1_reward: [-1635.0, -304.0, -233.0, -539.0]
      policy_agent-2_reward: [-714.0, -829.0, -223.0, -243.0]
      policy_agent-3_reward: [-1418.0, -776.0, -284.0, -204.0]
      policy_agent-4_reward: [-518.0, -792.0, -185.0, -228.0]
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -400.0
      agent-1: -233.0
      agent-2: -223.0
      agent-3: -204.0
      agent-4: -185.0
    policy_reward_mean:
      agent-0: -718.75
      agent-1: -677.75
      agent-2: -502.25
      agent-3: -670.5
      agent-4: -430.75
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.18426561130589345
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.078740745128364
      mean_inference_ms: 20.465832203546437
      mean_raw_obs_processing_ms: 2.4191114857079503
  time_since_restore: 112.48553395271301
  time_this_iter_s: 5.0276429653167725
  time_total_s: 112.48553395271301
  timers:
    apply_grad_throughput: 2350.878
    apply_grad_time_ms: 21.269
    grad_wait_time_ms: 0.147
    synch_weights_time_ms: 6.026
    training_iteration_time_ms: 0.159
  timestamp: 1661790102
  timesteps_since_restore: 0
  timesteps_total: 4720
  training_iteration: 22
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:21:47 (running for 00:02:30.84)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |     22 |          112.486 | 4720 |    -3000 |                      0 |                -1449 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 24700
  counters:
    num_agent_steps_sampled: 24700
    num_agent_steps_trained: 24700
    num_env_steps_sampled: 4940
    num_env_steps_trained: 4940
  custom_metrics:
    agent-0-apples_consumed: [4, 0, 0, 7]
    agent-0-beam_fired: [145, 98, 74, 63]
    agent-0-beam_hit: [24, 4, 9, 7]
    agent-0-cleaning: [103.0, 105.0, 110.0, 107.0]
    agent-1-apples_consumed: [0, 9, 12, 5]
    agent-1-beam_fired: [83, 63, 45, 43]
    agent-1-beam_hit: [29, 5, 4, 9]
    agent-1-cleaning: [105.0, 87.0, 74.0, 57.0]
    agent-2-apples_consumed: [2, 4, 2, 7]
    agent-2-beam_fired: [115, 81, 75, 50]
    agent-2-beam_hit: [11, 13, 3, 4]
    agent-2-cleaning: [121.0, 135.0, 157.0, 128.0]
    agent-3-apples_consumed: [0, 15, 0, 1]
    agent-3-beam_fired: [115, 90, 83, 55]
    agent-3-beam_hit: [23, 11, 3, 3]
    agent-3-cleaning: [104.0, 85.0, 83.0, 109.0]
    agent-4-apples_consumed: [5, 14, 18, 16]
    agent-4-beam_fired: [72, 55, 53, 44]
    agent-4-beam_hit: [8, 14, 3, 4]
    agent-4-cleaning: [96.0, 122.0, 103.0, 146.0]
  date: 2022-08-29_17-21-47
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -1449.0
  episode_reward_mean: -3000.0
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 4
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner:
      agent-0:
        learner_stats:
          allreduce_latency: 0.0
          cur_lr: 0.0001
          entropy_coeff: 0.01
          grad_gnorm: 40.0
          policy_entropy: 20.24666976928711
          policy_loss: 0.6305694580078125
          vf_loss: 0.42824578285217285
      agent-1:
        learner_stats:
          allreduce_latency: 0.0
          cur_lr: 0.0001
          entropy_coeff: 0.01
          grad_gnorm: 40.0
          policy_entropy: 17.902114868164062
          policy_loss: 18.526538848876953
          vf_loss: 12.59398078918457
      agent-2:
        learner_stats:
          allreduce_latency: 0.0
          cur_lr: 0.0001
          entropy_coeff: 0.01
          grad_gnorm: 40.0
          policy_entropy: 18.612075805664062
          policy_loss: 4.894312858581543
          vf_loss: 1.0211248397827148
      agent-3:
        learner_stats:
          allreduce_latency: 0.0
          cur_lr: 0.0001
          entropy_coeff: 0.01
          grad_gnorm: 40.0
          policy_entropy: 19.16912841796875
          policy_loss: 9.275075912475586
          vf_loss: 1.4545429944992065
      agent-4:
        learner_stats:
          allreduce_latency: 0.0
          cur_lr: 0.0001
          entropy_coeff: 0.01
          grad_gnorm: 40.0
          policy_entropy: 18.061321258544922
          policy_loss: 22.94757652282715
          vf_loss: 12.18728256225586
    num_agent_steps_sampled: 24700
    num_agent_steps_trained: 24700
    num_env_steps_sampled: 4940
    num_env_steps_trained: 4940
  iterations_since_restore: 23
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 24700
  num_agent_steps_trained: 24700
  num_env_steps_sampled: 4940
  num_env_steps_sampled_this_iter: 220
  num_env_steps_trained: 4940
  num_env_steps_trained_this_iter: 220
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 220
  perf:
    cpu_util_percent: 10.766666666666666
    gpu_util_percent0: 0.0
    ram_util_percent: 24.03333333333333
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -400.0
    agent-1: -233.0
    agent-2: -223.0
    agent-3: -204.0
    agent-4: -185.0
  policy_reward_mean:
    agent-0: -718.75
    agent-1: -677.75
    agent-2: -502.25
    agent-3: -670.5
    agent-4: -430.75
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.18426561130589345
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.078740745128364
    mean_inference_ms: 20.465832203546437
    mean_raw_obs_processing_ms: 2.4191114857079503
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed: [4, 0, 0, 7]
      agent-0-beam_fired: [145, 98, 74, 63]
      agent-0-beam_hit: [24, 4, 9, 7]
      agent-0-cleaning: [103.0, 105.0, 110.0, 107.0]
      agent-1-apples_consumed: [0, 9, 12, 5]
      agent-1-beam_fired: [83, 63, 45, 43]
      agent-1-beam_hit: [29, 5, 4, 9]
      agent-1-cleaning: [105.0, 87.0, 74.0, 57.0]
      agent-2-apples_consumed: [2, 4, 2, 7]
      agent-2-beam_fired: [115, 81, 75, 50]
      agent-2-beam_hit: [11, 13, 3, 4]
      agent-2-cleaning: [121.0, 135.0, 157.0, 128.0]
      agent-3-apples_consumed: [0, 15, 0, 1]
      agent-3-beam_fired: [115, 90, 83, 55]
      agent-3-beam_hit: [23, 11, 3, 3]
      agent-3-cleaning: [104.0, 85.0, 83.0, 109.0]
      agent-4-apples_consumed: [5, 14, 18, 16]
      agent-4-beam_fired: [72, 55, 53, 44]
      agent-4-beam_hit: [8, 14, 3, 4]
      agent-4-cleaning: [96.0, 122.0, 103.0, 146.0]
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -1449.0
    episode_reward_mean: -3000.0
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths: [1000, 1000, 1000, 1000]
      episode_reward: [-5779.0, -3101.0, -1449.0, -1671.0]
      policy_agent-0_reward: [-1494.0, -400.0, -524.0, -457.0]
      policy_agent-1_reward: [-1635.0, -304.0, -233.0, -539.0]
      policy_agent-2_reward: [-714.0, -829.0, -223.0, -243.0]
      policy_agent-3_reward: [-1418.0, -776.0, -284.0, -204.0]
      policy_agent-4_reward: [-518.0, -792.0, -185.0, -228.0]
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -400.0
      agent-1: -233.0
      agent-2: -223.0
      agent-3: -204.0
      agent-4: -185.0
    policy_reward_mean:
      agent-0: -718.75
      agent-1: -677.75
      agent-2: -502.25
      agent-3: -670.5
      agent-4: -430.75
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.18426561130589345
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.078740745128364
      mean_inference_ms: 20.465832203546437
      mean_raw_obs_processing_ms: 2.4191114857079503
  time_since_restore: 117.50000071525574
  time_this_iter_s: 5.014466762542725
  time_total_s: 117.50000071525574
  timers:
    apply_grad_throughput: 2386.183
    apply_grad_time_ms: 20.954
    grad_wait_time_ms: 0.745
    synch_weights_time_ms: 6.028
    training_iteration_time_ms: 3.66
  timestamp: 1661790107
  timesteps_since_restore: 0
  timesteps_total: 4940
  training_iteration: 23
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

(RolloutWorker pid=18610) check agent ids ['agent-0', 'agent-1', 'agent-2', 'agent-3', 'agent-4']
(RolloutWorker pid=18610) checking again ['agent-0', 'agent-1', 'agent-2', 'agent-3', 'agent-4']
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) ptinting type <class 'numpy.int32'>
(RolloutWorker pid=18610) type dawg <class 'int'>
(RolloutWorker pid=18610) here we are agent-4 [6, 8, 1, 8, 8, 6, 1, 5, 6, 0, 7, 1, 6, 1, 3, 8, 2, 5, 5, 4, 5, 2, 5, 8, 1, 2, 8, 8, 1, 5, 3, 8, 8, 1, 5, 1, 1, 5, 0, 5, 2, 8, 1, 5, 6, 6, 2, 2, 3, 1, 0, 7, 1, 5, 7, 5, 3, 1, 1, 4, 1, 3, 3, 5, 6, 3, 3, 3, 0, 5, 5, 6, 3, 3, 0, 0, 0, 4, 8, 5, 8, 3, 4, 0, 0, 0, 3, 1, 0, 5, 6, 8, 6, 5, 6, 6, 5, 5, 4, 3, 6, 0, 1, 6, 5, 8, 1, 8, 2, 4, 3, 3, 6, 1, 3, 6, 1, 0, 6, 5, 2, 1, 5, 5, 6, 3, 8, 1, 5, 6, 3, 3, 2, 3, 0, 3, 4, 3, 1, 0, 4, 1, 4, 3, 5, 2, 3, 3, 5, 3, 1, 1, 6, 6, 7, 5, 7, 6, 0, 3, 5, 3, 0, 0, 0, 8, 8, 3, 5, 3, 8, 6, 6, 8, 6, 5, 8, 7, 8, 1, 4, 8, 4, 3, 6, 6, 1, 0, 6, 6, 3, 8, 5, 8, 3, 0, 6, 0, 6, 2, 2, 1, 2, 5, 6, 3, 1, 4, 5, 3, 3, 1, 1, 1, 8, 4, 5, 1, 0, 6, 8, 0, 6, 4, 1, 6, 0, 3, 8, 6, 6, 6, 4, 0, 0, 0, 7, 6, 8, 8, 4, 4, 8, 4, 6, 6, 0, 5, 1, 1, 1, 1, 1, 3, 1, 5, 5, 5, 3, 5, 4, 2, 2, 3, 3, 4, 6, 2, 8, 2, 0, 3, 0, 5, 5, 5, 1, 6, 4, 0, 8, 0, 8, 0, 2, 5, 5, 6, 0, 5, 4, 6, 0, 1, 8, 0, 3, 5, 3, 0, 3, 5, 6, 8, 0, 0, 5, 0, 5, 0, 6, 3, 5, 1, 6, 8, 2, 8, 3, 2, 1, 1, 0, 3, 6, 1, 8, 5, 0, 3, 7, 7, 8, 1, 0, 3, 0, 5, 3, 2, 5, 6, 3, 6, 6, 1, 3, 2, 0, 8, 5, 2, 4, 8, 8, 2, 6, 4, 6, 6, 2, 8, 1, 6, 6, 1, 3, 1, 1, 4, 2, 6, 5, 7, 4, 5, 1, 1, 0, 1, 5, 3, 1, 3, 3, 0, 0, 1, 4, 0, 2, 3, 2, 4, 3, 8, 4, 6, 0, 6, 8, 4, 3, 2, 5, 4, 3, 1, 6, 8, 5, 3, 6, 3, 3, 1, 6, 2, 5, 8, 3, 5, 3, 8, 8, 3, 2, 1, 3, 2, 6, 3, 8, 4, 5, 8, 5, 5, 0, 1, 0, 5, 3, 3, 3, 2, 4, 4, 6, 1, 6, 1, 0, 8, 8, 2, 6, 7, 0, 1, 1, 8, 3, 2, 6, 5, 3, 0, 5, 5, 0, 3, 1, 0, 3, 3, 0, 0, 0, 3, 4, 6, 3, 6, 1, 8, 0, 0, 3, 2, 0, 1, 1, 5, 6, 1, 0, 5, 4, 4, 0, 4, 4, 3, 0, 4, 0, 5, 0, 3, 8, 1, 1, 3, 8, 3, 0, 2, 0, 1, 0, 5, 0, 8, 3, 3, 6, 4, 3, 3, 1, 6, 0, 0, 4, 3, 3, 8, 6, 8, 1, 0, 2, 4, 6, 5, 1, 1, 3, 8, 5, 3, 0, 1, 8, 1, 7, 2, 2, 6, 2, 4, 1, 5, 5, 3, 0, 3, 4, 2, 4, 3, 5, 5, 5, 1, 6, 3, 8, 4, 0, 8, 5, 4, 8, 3, 3, 3, 6, 3, 1, 6, 2, 8, 1, 1, 2, 1, 8, 0, 1, 8, 8, 2, 1, 5, 0, 3, 3, 3, 5, 0, 5, 4, 8, 4, 4, 8, 2, 3, 8, 3, 5, 8, 1, 8, 2, 5, 0, 5, 2, 4, 1, 0, 5, 1, 1, 6, 8, 5, 1, 8, 1, 6, 2, 6, 5, 3, 5, 2, 0, 3, 5, 6, 8, 0, 3, 1, 5, 2, 1, 6, 1, 3, 1, 3, 4, 6, 5, 0, 0, 3, 6, 5, 0, 1, 3, 5, 0, 1, 1, 0, 8, 8, 1, 7, 8, 8, 6, 6, 1, 3, 3, 7, 1, 8, 3, 1, 1, 8, 8, 1, 6, 6, 3, 3, 3, 4, 5, 6, 0, 1, 3, 1, 0, 2, 6, 4, 4, 6, 2, 0, 8, 5, 1, 3, 1, 6, 5, 2, 6, 1, 0, 7, 0, 0, 2, 5, 5, 4, 1, 0, 6, 4, 5, 6, 4, 6, 0, 5, 2, 6, 0, 3, 1, 6, 0, 0, 0, 2, 6, 1, 7, 7, 5, 2, 2, 1, 3, 5, 6, 5, 6, 5, 0, 1, 6, 7, 7, 6, 5, 5, 5, 4, 3, 3, 3, 8, 5, 3, 3, 1, 0, 8, 5, 6, 5, 6, 4, 3, 1, 6, 3, 5, 5, 7, 0, 6, 8, 5, 0, 1, 0, 2, 0, 1, 4, 3, 3, 1, 1, 0, 6, 6, 6, 4, 3, 1, 5, 5, 8, 1, 4, 6, 2, 1, 4, 2, 6, 3, 3, 4, 8, 1, 4, 6, 1, 7, 5, 3, 7, 3, 1, 1, 6, 3, 0, 2, 8, 2, 4, 3, 7, 3, 6, 4, 8, 1, 0, 8, 8, 1, 8, 8, 1, 8, 8, 5, 1, 0, 5, 6, 8, 4, 8, 6, 3, 2, 3, 4, 3, 3, 3, 8, 0, 1, 0, 0, 7, 5, 5, 8, 3, 3, 1, 5, 0, 4, 8, 1, 1, 1, 3, 8, 2, 5, 1, 8, 8, 1, 8, 7, 2, 2, 3, 1, 0, 3, 2, 7, 0, 0, 3, 0, 0, 3, 0, 3, 1, 2, 5, 4, 3, 0, 1, 3, 4, 0, 8, 6, 3, 3, 6, 1, 8, 8, 3, 3, 3, 3, 4, 5, 3, 6, 2, 8, 4, 2, 7, 2, 1, 1, 4, 0, 3, 3, 7, 8, 3, 4, 4, 3, 3, 5, 2, 1, 2, 6, 5, 0, 0, 1, 0, 6, 6, 6, 5, 3, 1, 6]
(RolloutWorker pid=18610) and here {'agent-0-beam_fired': 47, 'agent-0-cleaning': 135.0, 'agent-0-beam_hit': 2, 'agent-0-apples_consumed': 4, 'agent-1-beam_fired': 28, 'agent-1-cleaning': 57.0, 'agent-1-beam_hit': 8, 'agent-1-apples_consumed': 10, 'agent-2-beam_fired': 49, 'agent-2-cleaning': 125.0, 'agent-2-beam_hit': 2, 'agent-2-apples_consumed': 0, 'agent-3-beam_fired': 77, 'agent-3-cleaning': 104.0, 'agent-3-beam_hit': 5, 'agent-3-apples_consumed': 5, 'agent-4-beam_fired': 28, 'agent-4-cleaning': 113.0, 'agent-4-beam_hit': 5, 'agent-4-apples_consumed': 21}
(RolloutWorker pid=18610) episode 777382411 (env-idx=0) started.
(RolloutWorker pid=18610) agent id  agent-0
(RolloutWorker pid=18610) agent id  agent-1
(RolloutWorker pid=18610) agent id  agent-2
(RolloutWorker pid=18610) agent id  agent-3
(RolloutWorker pid=18610) agent id  agent-4
== Status ==
Current time: 2022-08-29 17:21:52 (running for 00:02:35.99)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |     23 |            117.5 | 4940 |    -3000 |                      0 |                -1449 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 25750
  counters:
    num_agent_steps_sampled: 25750
    num_agent_steps_trained: 25750
    num_env_steps_sampled: 5150
    num_env_steps_trained: 5150
  custom_metrics:
    agent-0-apples_consumed: [4, 0, 0, 7, 4]
    agent-0-beam_fired: [145, 98, 74, 63, 47]
    agent-0-beam_hit: [24, 4, 9, 7, 2]
    agent-0-cleaning: [103.0, 105.0, 110.0, 107.0, 135.0]
    agent-1-apples_consumed: [0, 9, 12, 5, 10]
    agent-1-beam_fired: [83, 63, 45, 43, 28]
    agent-1-beam_hit: [29, 5, 4, 9, 8]
    agent-1-cleaning: [105.0, 87.0, 74.0, 57.0, 57.0]
    agent-2-apples_consumed: [2, 4, 2, 7, 0]
    agent-2-beam_fired: [115, 81, 75, 50, 49]
    agent-2-beam_hit: [11, 13, 3, 4, 2]
    agent-2-cleaning: [121.0, 135.0, 157.0, 128.0, 125.0]
    agent-3-apples_consumed: [0, 15, 0, 1, 5]
    agent-3-beam_fired: [115, 90, 83, 55, 77]
    agent-3-beam_hit: [23, 11, 3, 3, 5]
    agent-3-cleaning: [104.0, 85.0, 83.0, 109.0, 104.0]
    agent-4-apples_consumed: [5, 14, 18, 16, 21]
    agent-4-beam_fired: [72, 55, 53, 44, 28]
    agent-4-beam_hit: [8, 14, 3, 4, 5]
    agent-4-cleaning: [96.0, 122.0, 103.0, 146.0, 113.0]
  date: 2022-08-29_17-21-52
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -1338.0
  episode_reward_mean: -2667.6
  episode_reward_min: -5779.0
  episodes_this_iter: 1
  episodes_total: 5
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 25750
    num_agent_steps_trained: 25750
    num_env_steps_sampled: 5150
    num_env_steps_trained: 5150
  iterations_since_restore: 24
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 25750
  num_agent_steps_trained: 25750
  num_env_steps_sampled: 5150
  num_env_steps_sampled_this_iter: 210
  num_env_steps_trained: 5150
  num_env_steps_trained_this_iter: 210
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 210
  perf:
    cpu_util_percent: 10.799999999999999
    gpu_util_percent0: 0.0
    ram_util_percent: 23.933333333333334
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -143.0
    agent-1: -233.0
    agent-2: -149.0
    agent-3: -204.0
    agent-4: -185.0
  policy_reward_mean:
    agent-0: -603.6
    agent-1: -635.6
    agent-2: -431.6
    agent-3: -600.8
    agent-4: -396.0
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.1845749679850442
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.077011113965765
    mean_inference_ms: 20.5163691507535
    mean_raw_obs_processing_ms: 2.423818526882139
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed: [4, 0, 0, 7, 4]
      agent-0-beam_fired: [145, 98, 74, 63, 47]
      agent-0-beam_hit: [24, 4, 9, 7, 2]
      agent-0-cleaning: [103.0, 105.0, 110.0, 107.0, 135.0]
      agent-1-apples_consumed: [0, 9, 12, 5, 10]
      agent-1-beam_fired: [83, 63, 45, 43, 28]
      agent-1-beam_hit: [29, 5, 4, 9, 8]
      agent-1-cleaning: [105.0, 87.0, 74.0, 57.0, 57.0]
      agent-2-apples_consumed: [2, 4, 2, 7, 0]
      agent-2-beam_fired: [115, 81, 75, 50, 49]
      agent-2-beam_hit: [11, 13, 3, 4, 2]
      agent-2-cleaning: [121.0, 135.0, 157.0, 128.0, 125.0]
      agent-3-apples_consumed: [0, 15, 0, 1, 5]
      agent-3-beam_fired: [115, 90, 83, 55, 77]
      agent-3-beam_hit: [23, 11, 3, 3, 5]
      agent-3-cleaning: [104.0, 85.0, 83.0, 109.0, 104.0]
      agent-4-apples_consumed: [5, 14, 18, 16, 21]
      agent-4-beam_fired: [72, 55, 53, 44, 28]
      agent-4-beam_hit: [8, 14, 3, 4, 5]
      agent-4-cleaning: [96.0, 122.0, 103.0, 146.0, 113.0]
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -1338.0
    episode_reward_mean: -2667.6
    episode_reward_min: -5779.0
    episodes_this_iter: 1
    hist_stats:
      episode_lengths: [1000, 1000, 1000, 1000, 1000]
      episode_reward: [-5779.0, -3101.0, -1449.0, -1671.0, -1338.0]
      policy_agent-0_reward: [-1494.0, -400.0, -524.0, -457.0, -143.0]
      policy_agent-1_reward: [-1635.0, -304.0, -233.0, -539.0, -467.0]
      policy_agent-2_reward: [-714.0, -829.0, -223.0, -243.0, -149.0]
      policy_agent-3_reward: [-1418.0, -776.0, -284.0, -204.0, -322.0]
      policy_agent-4_reward: [-518.0, -792.0, -185.0, -228.0, -257.0]
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -143.0
      agent-1: -233.0
      agent-2: -149.0
      agent-3: -204.0
      agent-4: -185.0
    policy_reward_mean:
      agent-0: -603.6
      agent-1: -635.6
      agent-2: -431.6
      agent-3: -600.8
      agent-4: -396.0
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.1845749679850442
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.077011113965765
      mean_inference_ms: 20.5163691507535
      mean_raw_obs_processing_ms: 2.423818526882139
  time_since_restore: 122.54263949394226
  time_this_iter_s: 5.042638778686523
  time_total_s: 122.54263949394226
  timers:
    apply_grad_throughput: 2484.009
    apply_grad_time_ms: 20.129
    grad_wait_time_ms: 0.122
    synch_weights_time_ms: 5.464
    training_iteration_time_ms: 0.131
  timestamp: 1661790112
  timesteps_since_restore: 0
  timesteps_total: 5150
  training_iteration: 24
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:21:58 (running for 00:02:41.23)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |     24 |          122.543 | 5150 |  -2667.6 |                      0 |                -1338 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 26850
  counters:
    num_agent_steps_sampled: 26850
    num_agent_steps_trained: 26850
    num_env_steps_sampled: 5370
    num_env_steps_trained: 5370
  custom_metrics:
    agent-0-apples_consumed: [4, 0, 0, 7, 4]
    agent-0-beam_fired: [145, 98, 74, 63, 47]
    agent-0-beam_hit: [24, 4, 9, 7, 2]
    agent-0-cleaning: [103.0, 105.0, 110.0, 107.0, 135.0]
    agent-1-apples_consumed: [0, 9, 12, 5, 10]
    agent-1-beam_fired: [83, 63, 45, 43, 28]
    agent-1-beam_hit: [29, 5, 4, 9, 8]
    agent-1-cleaning: [105.0, 87.0, 74.0, 57.0, 57.0]
    agent-2-apples_consumed: [2, 4, 2, 7, 0]
    agent-2-beam_fired: [115, 81, 75, 50, 49]
    agent-2-beam_hit: [11, 13, 3, 4, 2]
    agent-2-cleaning: [121.0, 135.0, 157.0, 128.0, 125.0]
    agent-3-apples_consumed: [0, 15, 0, 1, 5]
    agent-3-beam_fired: [115, 90, 83, 55, 77]
    agent-3-beam_hit: [23, 11, 3, 3, 5]
    agent-3-cleaning: [104.0, 85.0, 83.0, 109.0, 104.0]
    agent-4-apples_consumed: [5, 14, 18, 16, 21]
    agent-4-beam_fired: [72, 55, 53, 44, 28]
    agent-4-beam_hit: [8, 14, 3, 4, 5]
    agent-4-cleaning: [96.0, 122.0, 103.0, 146.0, 113.0]
  date: 2022-08-29_17-21-58
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -1338.0
  episode_reward_mean: -2667.6
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 5
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 26850
    num_agent_steps_trained: 26850
    num_env_steps_sampled: 5370
    num_env_steps_trained: 5370
  iterations_since_restore: 25
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 26850
  num_agent_steps_trained: 26850
  num_env_steps_sampled: 5370
  num_env_steps_sampled_this_iter: 220
  num_env_steps_trained: 5370
  num_env_steps_trained_this_iter: 220
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 220
  perf:
    cpu_util_percent: 10.433333333333332
    gpu_util_percent0: 0.0
    ram_util_percent: 24.0
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -143.0
    agent-1: -233.0
    agent-2: -149.0
    agent-3: -204.0
    agent-4: -185.0
  policy_reward_mean:
    agent-0: -603.6
    agent-1: -635.6
    agent-2: -431.6
    agent-3: -600.8
    agent-4: -396.0
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.1845749679850442
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.077011113965765
    mean_inference_ms: 20.5163691507535
    mean_raw_obs_processing_ms: 2.423818526882139
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed: [4, 0, 0, 7, 4]
      agent-0-beam_fired: [145, 98, 74, 63, 47]
      agent-0-beam_hit: [24, 4, 9, 7, 2]
      agent-0-cleaning: [103.0, 105.0, 110.0, 107.0, 135.0]
      agent-1-apples_consumed: [0, 9, 12, 5, 10]
      agent-1-beam_fired: [83, 63, 45, 43, 28]
      agent-1-beam_hit: [29, 5, 4, 9, 8]
      agent-1-cleaning: [105.0, 87.0, 74.0, 57.0, 57.0]
      agent-2-apples_consumed: [2, 4, 2, 7, 0]
      agent-2-beam_fired: [115, 81, 75, 50, 49]
      agent-2-beam_hit: [11, 13, 3, 4, 2]
      agent-2-cleaning: [121.0, 135.0, 157.0, 128.0, 125.0]
      agent-3-apples_consumed: [0, 15, 0, 1, 5]
      agent-3-beam_fired: [115, 90, 83, 55, 77]
      agent-3-beam_hit: [23, 11, 3, 3, 5]
      agent-3-cleaning: [104.0, 85.0, 83.0, 109.0, 104.0]
      agent-4-apples_consumed: [5, 14, 18, 16, 21]
      agent-4-beam_fired: [72, 55, 53, 44, 28]
      agent-4-beam_hit: [8, 14, 3, 4, 5]
      agent-4-cleaning: [96.0, 122.0, 103.0, 146.0, 113.0]
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -1338.0
    episode_reward_mean: -2667.6
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths: [1000, 1000, 1000, 1000, 1000]
      episode_reward: [-5779.0, -3101.0, -1449.0, -1671.0, -1338.0]
      policy_agent-0_reward: [-1494.0, -400.0, -524.0, -457.0, -143.0]
      policy_agent-1_reward: [-1635.0, -304.0, -233.0, -539.0, -467.0]
      policy_agent-2_reward: [-714.0, -829.0, -223.0, -243.0, -149.0]
      policy_agent-3_reward: [-1418.0, -776.0, -284.0, -204.0, -322.0]
      policy_agent-4_reward: [-518.0, -792.0, -185.0, -228.0, -257.0]
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -143.0
      agent-1: -233.0
      agent-2: -149.0
      agent-3: -204.0
      agent-4: -185.0
    policy_reward_mean:
      agent-0: -603.6
      agent-1: -635.6
      agent-2: -431.6
      agent-3: -600.8
      agent-4: -396.0
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.1845749679850442
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.077011113965765
      mean_inference_ms: 20.5163691507535
      mean_raw_obs_processing_ms: 2.423818526882139
  time_since_restore: 127.68229556083679
  time_this_iter_s: 5.139656066894531
  time_total_s: 127.68229556083679
  timers:
    apply_grad_throughput: 2461.351
    apply_grad_time_ms: 20.314
    grad_wait_time_ms: 0.142
    synch_weights_time_ms: 5.778
    training_iteration_time_ms: 0.153
  timestamp: 1661790118
  timesteps_since_restore: 0
  timesteps_total: 5370
  training_iteration: 25
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542

== Status ==
Current time: 2022-08-29 17:22:03 (running for 00:02:46.41)
Memory usage on this node: 3.7/15.5 GiB
Using FIFO scheduling algorithm.
Resources requested: 2.0/20 CPUs, 0/1 GPUs, 0.0/8.81 GiB heap, 0.0/4.4 GiB objects (0.0/1.0 accelerator_type:G)
Result logdir: /mnt/c/Users/timf3/PycharmProjects/SSD2/ray_results/cleanup/testingA3C
Number of trials: 1/1 (1 RUNNING)
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+
| Trial name              | status   | loc                 |   iter |   total time (s) |   ts |   reward |   num_recreated_wor... |   episode_reward_max |   episode_reward_min |
|-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------|
| A3C_cleanup_54261_00000 | RUNNING  | 172.17.250.69:18555 |     25 |          127.682 | 5370 |  -2667.6 |                      0 |                -1338 |                -5779 |
+-------------------------+----------+---------------------+--------+------------------+------+----------+------------------------+----------------------+----------------------+


Result for A3C_cleanup_54261_00000:
  agent_timesteps_total: 27950
  counters:
    num_agent_steps_sampled: 27950
    num_agent_steps_trained: 27950
    num_env_steps_sampled: 5590
    num_env_steps_trained: 5590
  custom_metrics:
    agent-0-apples_consumed: [4, 0, 0, 7, 4]
    agent-0-beam_fired: [145, 98, 74, 63, 47]
    agent-0-beam_hit: [24, 4, 9, 7, 2]
    agent-0-cleaning: [103.0, 105.0, 110.0, 107.0, 135.0]
    agent-1-apples_consumed: [0, 9, 12, 5, 10]
    agent-1-beam_fired: [83, 63, 45, 43, 28]
    agent-1-beam_hit: [29, 5, 4, 9, 8]
    agent-1-cleaning: [105.0, 87.0, 74.0, 57.0, 57.0]
    agent-2-apples_consumed: [2, 4, 2, 7, 0]
    agent-2-beam_fired: [115, 81, 75, 50, 49]
    agent-2-beam_hit: [11, 13, 3, 4, 2]
    agent-2-cleaning: [121.0, 135.0, 157.0, 128.0, 125.0]
    agent-3-apples_consumed: [0, 15, 0, 1, 5]
    agent-3-beam_fired: [115, 90, 83, 55, 77]
    agent-3-beam_hit: [23, 11, 3, 3, 5]
    agent-3-cleaning: [104.0, 85.0, 83.0, 109.0, 104.0]
    agent-4-apples_consumed: [5, 14, 18, 16, 21]
    agent-4-beam_fired: [72, 55, 53, 44, 28]
    agent-4-beam_hit: [8, 14, 3, 4, 5]
    agent-4-cleaning: [96.0, 122.0, 103.0, 146.0, 113.0]
  date: 2022-08-29_17-22-03
  done: false
  episode_len_mean: 1000.0
  episode_media: {}
  episode_reward_max: -1338.0
  episode_reward_mean: -2667.6
  episode_reward_min: -5779.0
  episodes_this_iter: 0
  episodes_total: 5
  experiment_id: ee29a8f6539749f6813ceea83cbb042d
  hostname: timf34
  info:
    learner: {}
    num_agent_steps_sampled: 27950
    num_agent_steps_trained: 27950
    num_env_steps_sampled: 5590
    num_env_steps_trained: 5590
  iterations_since_restore: 26
  node_ip: 172.17.250.69
  num_agent_steps_sampled: 27950
  num_agent_steps_trained: 27950
  num_env_steps_sampled: 5590
  num_env_steps_sampled_this_iter: 220
  num_env_steps_trained: 5590
  num_env_steps_trained_this_iter: 220
  num_faulty_episodes: 0
  num_healthy_workers: 1
  num_recreated_workers: 0
  num_steps_trained_this_iter: 220
  perf:
    cpu_util_percent: 10.966666666666667
    gpu_util_percent0: 0.0
    ram_util_percent: 24.066666666666666
    vram_util_percent0: 0.0
  pid: 18555
  policy_reward_max:
    agent-0: -143.0
    agent-1: -233.0
    agent-2: -149.0
    agent-3: -204.0
    agent-4: -185.0
  policy_reward_mean:
    agent-0: -603.6
    agent-1: -635.6
    agent-2: -431.6
    agent-3: -600.8
    agent-4: -396.0
  policy_reward_min:
    agent-0: -1494.0
    agent-1: -1635.0
    agent-2: -829.0
    agent-3: -1418.0
    agent-4: -792.0
  sampler_perf:
    mean_action_processing_ms: 0.1845749679850442
    mean_env_render_ms: 0.0
    mean_env_wait_ms: 1.077011113965765
    mean_inference_ms: 20.5163691507535
    mean_raw_obs_processing_ms: 2.423818526882139
  sampler_results:
    custom_metrics:
      agent-0-apples_consumed: [4, 0, 0, 7, 4]
      agent-0-beam_fired: [145, 98, 74, 63, 47]
      agent-0-beam_hit: [24, 4, 9, 7, 2]
      agent-0-cleaning: [103.0, 105.0, 110.0, 107.0, 135.0]
      agent-1-apples_consumed: [0, 9, 12, 5, 10]
      agent-1-beam_fired: [83, 63, 45, 43, 28]
      agent-1-beam_hit: [29, 5, 4, 9, 8]
      agent-1-cleaning: [105.0, 87.0, 74.0, 57.0, 57.0]
      agent-2-apples_consumed: [2, 4, 2, 7, 0]
      agent-2-beam_fired: [115, 81, 75, 50, 49]
      agent-2-beam_hit: [11, 13, 3, 4, 2]
      agent-2-cleaning: [121.0, 135.0, 157.0, 128.0, 125.0]
      agent-3-apples_consumed: [0, 15, 0, 1, 5]
      agent-3-beam_fired: [115, 90, 83, 55, 77]
      agent-3-beam_hit: [23, 11, 3, 3, 5]
      agent-3-cleaning: [104.0, 85.0, 83.0, 109.0, 104.0]
      agent-4-apples_consumed: [5, 14, 18, 16, 21]
      agent-4-beam_fired: [72, 55, 53, 44, 28]
      agent-4-beam_hit: [8, 14, 3, 4, 5]
      agent-4-cleaning: [96.0, 122.0, 103.0, 146.0, 113.0]
    episode_len_mean: 1000.0
    episode_media: {}
    episode_reward_max: -1338.0
    episode_reward_mean: -2667.6
    episode_reward_min: -5779.0
    episodes_this_iter: 0
    hist_stats:
      episode_lengths: [1000, 1000, 1000, 1000, 1000]
      episode_reward: [-5779.0, -3101.0, -1449.0, -1671.0, -1338.0]
      policy_agent-0_reward: [-1494.0, -400.0, -524.0, -457.0, -143.0]
      policy_agent-1_reward: [-1635.0, -304.0, -233.0, -539.0, -467.0]
      policy_agent-2_reward: [-714.0, -829.0, -223.0, -243.0, -149.0]
      policy_agent-3_reward: [-1418.0, -776.0, -284.0, -204.0, -322.0]
      policy_agent-4_reward: [-518.0, -792.0, -185.0, -228.0, -257.0]
    num_faulty_episodes: 0
    policy_reward_max:
      agent-0: -143.0
      agent-1: -233.0
      agent-2: -149.0
      agent-3: -204.0
      agent-4: -185.0
    policy_reward_mean:
      agent-0: -603.6
      agent-1: -635.6
      agent-2: -431.6
      agent-3: -600.8
      agent-4: -396.0
    policy_reward_min:
      agent-0: -1494.0
      agent-1: -1635.0
      agent-2: -829.0
      agent-3: -1418.0
      agent-4: -792.0
    sampler_perf:
      mean_action_processing_ms: 0.1845749679850442
      mean_env_render_ms: 0.0
      mean_env_wait_ms: 1.077011113965765
      mean_inference_ms: 20.5163691507535
      mean_raw_obs_processing_ms: 2.423818526882139
  time_since_restore: 132.8253185749054
  time_this_iter_s: 5.1430230140686035
  time_total_s: 132.8253185749054
  timers:
    apply_grad_throughput: 2563.486
    apply_grad_time_ms: 19.505
    grad_wait_time_ms: 0.16
    synch_weights_time_ms: 5.63
    training_iteration_time_ms: 0.171
  timestamp: 1661790123
  timesteps_since_restore: 0
  timesteps_total: 5590
  training_iteration: 26
  trial_id: '54261_00000'
  warmup_time: 16.466658353805542


Process finished with exit code -1



